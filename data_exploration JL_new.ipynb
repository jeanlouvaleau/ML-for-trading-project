{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663276e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cd8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2423b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score,accuracy_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV, MultiTaskLassoCV, MultiTaskElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor,MultiOutputClassifier\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371adc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce457d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f109b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f13923",
   "metadata": {},
   "source": [
    "# I- Training linear models on daily data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261e7b5",
   "metadata": {},
   "source": [
    "We will try to train models on both daily and weekly data. We may see some different dynamics between daily and weekly data: it could be easier to train a model on weekly data because there is less autocorrelation between datapoints. However in the meanwhile, the dataset on daily datapoints may be bigger so it could also be easier to train models on daily data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e53cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join('data', 'US', 'us_data.csv')\n",
    "dus = pd.read_csv(datapath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_inf = dus.columns[np.isinf(dus.to_numpy()).any(axis=0)]\n",
    "print(cols_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus.hist(figsize=(23, 23), bins=100)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec41c5",
   "metadata": {},
   "source": [
    "### A) Creating new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86313b39",
   "metadata": {},
   "source": [
    "Now we need to add the lagged values of yields as features. We have to choose lags and yields to add as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for USyield in ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3','DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30']:\n",
    "    series = dus[USyield]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "    plot_acf(series, lags=90,title = f'ACF {USyield}', ax = axes[0])\n",
    "    plot_pacf(series, lags=90,title = f'PACF {USyield}',ax = axes[1])\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0.5, 90)  # décale le début après 0\n",
    "        ax.set_ylim(-0.2, 0.2) \n",
    "\n",
    "        if USyield in ['DGS5','DGS7','DGS10', 'DGS20', 'DGS30']:\n",
    "            \n",
    "            ax.axvline(x=25, color='red', linestyle='--', linewidth=1)\n",
    "            ax.axvline(x=50, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa2f66",
   "metadata": {},
   "source": [
    "- We can see that on short term yields, there is much more autocorrelation in the data, up to more than 30 days. Returns in the past few days are highly correlated to returns in the next days. \n",
    "- However, on long term yields, there is much less autocorrelation and returns in the past 2 days are only slightly correlated to next day return. Surprisingly we see some persistent autocorrelation between returns at day t and t-25 and t-50. \n",
    "\n",
    "For maturities less than 1y, we'll add the following lags:\n",
    "- t-1,t-2,t-5,t-10,t-15,t-20,t-25,t-30,t-40,t-50\n",
    "\n",
    "For maturities more than 1y, we will add:\n",
    "- t-1,t-2,t-10,t-25,t-50\n",
    "\n",
    "We will probably need to do some PCA to combine features as they will be very correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3581d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1,2,5,10,15,20,25,30,40,50]:\n",
    "    dus[f'DGS1MO_t-{lag}'] = dus['DGS1MO'].shift(lag-1)\n",
    "    dus[f'DGS3MO_t-{lag}'] = dus['DGS3MO'].shift(lag-1)\n",
    "    dus[f'DGS6MO_t-{lag}'] = dus['DGS6MO'].shift(lag-1)\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "  \n",
    "\n",
    "for lag in [1,2,10,15,25,50]:\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "    dus[f'DGS2_t-{lag}'] = dus['DGS2'].shift(lag-1)\n",
    "    dus[f'DGS3_t-{lag}'] = dus['DGS3'].shift(lag-1)\n",
    "    dus[f'DGS5_t-{lag}'] = dus['DGS5'].shift(lag-1)\n",
    "    dus[f'DGS7_t-{lag}'] = dus['DGS7'].shift(lag-1)\n",
    "    dus[f'DGS10_t-{lag}'] = dus['DGS10'].shift(lag-1)\n",
    "    dus[f'DGS20_t-{lag}'] = dus['DGS20'].shift(lag-1)\n",
    "    dus[f'DGS30_t-{lag}'] = dus['DGS30'].shift(lag-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables to forecast \n",
    "\n",
    "dus['Y_1MO'] = dus['DGS1MO'].shift(-1)\n",
    "dus['Y_3MO'] = dus['DGS3MO'].shift(-1)\n",
    "dus['Y_6MO'] = dus['DGS6MO'].shift(-1)\n",
    "dus['Y_1year'] = dus['DGS1'].shift(-1)\n",
    "dus['Y_2year'] = dus['DGS2'].shift(-1)\n",
    "dus['Y_3year'] = dus['DGS3'].shift(-1)\n",
    "dus['Y_5year'] = dus['DGS5'].shift(-1)\n",
    "dus['Y_7year'] = dus['DGS7'].shift(-1)\n",
    "dus['Y_10year'] = dus['DGS10'].shift(-1)\n",
    "dus['Y_20year'] = dus['DGS20'].shift(-1)\n",
    "dus['Y_30year'] = dus['DGS30'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now remove the original yield columns\n",
    "dus = dus.drop(columns=['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3','DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c8c90",
   "metadata": {},
   "source": [
    "We can now look at the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,25))\n",
    "sns.heatmap(dus.corr(), cmap='seismic', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514c74a",
   "metadata": {},
   "source": [
    "Overall the features have a very low correlation with the target, so we'll remove the least correlated ones: \n",
    "\n",
    "- the lagged features that have a correlation coefficient < 0.05 in absolute value with all target variables. \n",
    "- the other features that have a correlation coefficient < 0.03 in absolute value with all target variables. We do a distinction between lagged features and other features because filtering all features with the 0.05 threshold removes somes features that should have a predictive impact: sp500, gold, VIX for instance. \n",
    "\n",
    "Moreover, given the very high correlation between lagged features, we'll apply a PCA in the pipeline on those to limit the number of colinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0962fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dus[['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']]\n",
    "dus_lagged_features = dus[[col for col in dus.columns if '_t-' in col]]\n",
    "dus_other =dus.drop(columns=[col for col in dus.columns if '_t-' in col])\n",
    "\n",
    "corrs = pd.DataFrame({\n",
    "    target: dus_lagged_features.corrwith(Y[target]) for target in Y.columns\n",
    "}).abs()  \n",
    "\n",
    "# repérer les colonnes où la corrélation absolue < 0.05 pour toutes les targets\n",
    "mask = (corrs < 0.05).all(axis=1)\n",
    "low_corr_features = corrs.index[mask]\n",
    "\n",
    "# supprimer ces colonnes\n",
    "dus_filtered = dus.drop(columns=low_corr_features)\n",
    "\n",
    "\n",
    "\n",
    "corrs = pd.DataFrame({\n",
    "    target: dus_other.corrwith(Y[target]) for target in Y.columns\n",
    "}).abs()  \n",
    "\n",
    "# repérer les colonnes où la corrélation absolue < 0.05 pour toutes les targets\n",
    "mask = (corrs < 0.03).all(axis=1)\n",
    "low_corr_features_2 = corrs.index[mask]\n",
    "\n",
    "# supprimer ces colonnes\n",
    "dus_filtered = dus_filtered.drop(columns=low_corr_features_2)\n",
    "\n",
    "print(f\"{len(low_corr_features) + len(low_corr_features_2)} features were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_corr_features)\n",
    "print(low_corr_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdeb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(dus_filtered.corr(), cmap='seismic', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294fcf",
   "metadata": {},
   "source": [
    "### B) PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1768633",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_filtered = dus_filtered.dropna()\n",
    "X = dus_filtered[['USGOOD', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'USWTRADE',\n",
    "       'USFIRE', 'PERMIT', 'UMCSENT', 'M2SL', 'M2REAL', 'TOTRESNS', 'CPIAUCSL',\n",
    "       'CPIAPPSL', 'CPITRNSL', 'CUSR0000SAC', 'CPIULFSL', 'CUUR0000SA0L2',\n",
    "       'PCEPI', 'DNDGRG3M086SBEA', 'MTSDS133FMS', 'GFDEGDQ188S',\n",
    "       'IRLTLT01DEM156N', 'IRLTLT01JPM156N', 'IRLTLT01GBM156N',\n",
    "       'IRLTLT01CAM156N', 'IRLTLT01AUM156N', 'IRLTLT01FRM156N', 'NASDAQCOM',\n",
    "       'AAA', 'BAA', 'DEXCAUS', 'DEXUSAL', 'NFCI', 'FEDFUNDS', 'BOGMBASE',\n",
    "       'WSHOSHO', 'T5YIE', 'T10YIE', 'log return gold', 'log return sp500',\n",
    "       'DGS1MO_t-1', 'DGS3MO_t-1', 'DGS6MO_t-1', 'DGS1_t-1', 'DGS1MO_t-2',\n",
    "       'DGS3MO_t-2', 'DGS6MO_t-2', 'DGS1MO_t-5', 'DGS3MO_t-5', 'DGS6MO_t-5',\n",
    "       'DGS3MO_t-10', 'DGS6MO_t-10', 'DGS1_t-10', 'DGS1MO_t-15', 'DGS3MO_t-15',\n",
    "       'DGS6MO_t-15', 'DGS1MO_t-20', 'DGS3MO_t-20', 'DGS6MO_t-20', 'DGS1_t-20',\n",
    "       'DGS3MO_t-30', 'DGS6MO_t-40', 'DGS1MO_t-50', 'DGS2_t-1', 'DGS3_t-1',\n",
    "       'DGS10_t-1', 'DGS20_t-1', 'DGS30_t-1', 'DGS7_t-50', 'DGS10_t-50',\n",
    "       'DGS20_t-50', 'DGS30_t-50']]\n",
    "\n",
    "Y = dus_filtered[['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47038b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Number of components kept:\", pca.n_components_)\n",
    "print(\"cumulative explained variance :\", pca.explained_variance_ratio_.cumsum())\n",
    "print(\"Composantes principales (coefficients sur les features originales) :\")\n",
    "print(pca.components_)\n",
    "\n",
    "print(\"Exemple des nouvelles features transformées :\")\n",
    "print(X_pca[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculer la contribution absolue sur PC1 et PC2\n",
    "pc1, pc2 = np.abs(pca.components_[:2])\n",
    "importance = pc1 + pc2\n",
    "\n",
    "# garder les n features les plus importantes\n",
    "n = 30\n",
    "top_idx = np.argsort(importance)[-n:]\n",
    "top_labels = X.columns[top_idx]\n",
    "top_components = pca.components_[:2, top_idx]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "circle = plt.Circle((0,0), 1, color='gray', fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "\n",
    "for i, (x, y) in enumerate(zip(top_components[0,:], top_components[1,:])):\n",
    "    plt.arrow(0, 0, x, y, color='r', alpha=0.6, head_width=0.02)\n",
    "    plt.text(x*1.15, y*1.15, top_labels[i], color='b', ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Correlation circle (top features)\")\n",
    "plt.grid()\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af776ae0",
   "metadata": {},
   "source": [
    "We can see that features are overall very correlated, so we'll train some models with PCA and some without."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4584738",
   "metadata": {},
   "source": [
    "### C) Training a ridge model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f7a26",
   "metadata": {},
   "source": [
    "We do a walk forward cross validation:\n",
    "- we train our model on 4 years of data (= approximately 1000 data points)\n",
    "- we do prediction for the next month (21 days)\n",
    "- wa add a PCA to the pipeline to deal with correlated features. We'll also train a model without PCA to see how it changes the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "window_train = 252 *4\n",
    "window_pred = 21          \n",
    "alphas = np.logspace(-3, 3, 20)\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.95)),   \n",
    "    ('ridge', MultiOutputRegressor(RidgeCV(fit_intercept=False,alphas=alphas, cv=tscv)))\n",
    "])\n",
    "\n",
    "pipe_nopca = Pipeline([\n",
    "    ('scaler',StandardScaler()),   \n",
    "    ('ridge', MultiOutputRegressor(RidgeCV(fit_intercept=False,alphas=alphas, cv=tscv)))\n",
    "])\n",
    "\n",
    "preds = []\n",
    "dates_pred = []\n",
    "r2_is_list = []\n",
    "r2_os_list=[]\n",
    "hit_rate_list = []\n",
    "\n",
    "preds_nopca = []\n",
    "dates_pred_nopca = []\n",
    "r2_is_list_nopca = []\n",
    "r2_os_list_nopca=[]\n",
    "hit_rate_list_nopca = []\n",
    "\n",
    "for start in tqdm(range(0, len(X) - window_train - window_pred + 1, window_pred)):\n",
    "  \n",
    "    end_train = start + window_train\n",
    "    end_pred = end_train + window_pred\n",
    "\n",
    "    X_train = X.iloc[start:end_train]\n",
    "    Y_train = Y.iloc[start:end_train]\n",
    "\n",
    "    X_test = X.iloc[end_train:end_pred]\n",
    "    Y_test = Y.iloc[end_train:end_pred]\n",
    "\n",
    "\n",
    "    # model with PCA \n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    preds.append(Y_pred)\n",
    "    dates_pred.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred), axis=0)\n",
    "    r2_is_list.append(r2_is)\n",
    "    r2_os_list.append(r2_oos)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "\n",
    "\n",
    "    # model without PCA \n",
    "    pipe_nopca.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_nopca = pipe_nopca.predict(X_test)\n",
    "\n",
    "    preds_nopca.append(Y_pred)\n",
    "    dates_pred_nopca.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe_nopca.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred_nopca), axis=0)\n",
    "    r2_is_list_nopca.append(r2_is)\n",
    "    r2_os_list_nopca.append(r2_oos)\n",
    "    hit_rate_list_nopca.append(hit_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_is_df = pd.DataFrame(r2_is_list) \n",
    "r2_is_df_nopca = pd.DataFrame(r2_is_list_nopca) \n",
    "r2_is_df.columns = Y.columns\n",
    "r2_is_df_nopca.columns = Y.columns\n",
    "r2_is_df.index = [date[0] for date in dates_pred]\n",
    "r2_is_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "r2_os_df = pd.DataFrame(r2_os_list)  \n",
    "r2_os_df.columns = Y.columns\n",
    "r2_os_df.index = [date[0] for date in dates_pred]\n",
    "r2_os_df_nopca = pd.DataFrame(r2_os_list_nopca) \n",
    "r2_os_df_nopca.columns = Y.columns\n",
    "r2_os_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "hr = pd.DataFrame(hit_rate_list) \n",
    "hr.columns = Y.columns\n",
    "hr.index = [date[0] for date in dates_pred]\n",
    "hr_nopca = pd.DataFrame(hit_rate_list_nopca) \n",
    "hr_nopca.columns = Y.columns\n",
    "hr_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "plt.close('all')  # ferme toutes les figures existantes\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,2,figsize=(28,28))\n",
    "\n",
    "r2_is_df.plot(ax = ax[0,0],title = 'Evolution of in sample R2 per model with PCA, per sample')\n",
    "ax[0,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_is_df_nopca.plot(ax = ax[0,1],title = 'Evolution of in sample R2 per model without PCA, per sample')\n",
    "ax[0,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "r2_os_df.plot(ax = ax[1,0], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per model with PCA, per sample')\n",
    "ax[1,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_os_df_nopca.plot(ax = ax[1,1], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per model without PCA, per sample')\n",
    "ax[1,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "hr.plot(ax = ax[2,0], title = 'Evolution of out-sample hit rate per model with PCA, per sample')\n",
    "ax[2,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "hr_nopca.plot(ax = ax[2,1], title = 'Evolution of out-sample hit rate per model without PCA, per sample')\n",
    "ax[2,1].grid(True, axis='y', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7249b4",
   "metadata": {},
   "source": [
    "- When looking at the in sample R2 of our models, we see that the R2 is overall low, but that it is better on short term yields, ie from 1 month to 1 year, and significantly lower for long-term yields. \n",
    "- we see significant variations in R2 in 2011 and 2020, probably because of outliers. \n",
    "\n",
    "- The out of sample R2 is close to zero or even negative so there is no predictive power in our model.\n",
    "\n",
    "- adding a PCA in the pipeline do not change anything to the in sample R2. However, the hit rate is slightly less variable when adding the PCA, suggesting a bit less overfitting (although the out of sample results are as bad).\n",
    "- The hit rate seems to be close to 0.5 on average for all models and all samples. So it appears the model does not do anything better than predicting at random - it completely overfits the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5de84f",
   "metadata": {},
   "source": [
    "## Training Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-5, 2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 252 *4\n",
    "window_pred = 21          \n",
    "alphas = np.logspace(-5, 2, 20)\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.95)),   \n",
    "    ('lasso', MultiTaskLassoCV(fit_intercept=False,alphas=alphas, cv=tscv))\n",
    "])\n",
    "\n",
    "pipe_nopca = Pipeline([\n",
    "    ('scaler',StandardScaler()),   \n",
    "    ('lasso', MultiTaskLassoCV(fit_intercept=False,alphas=alphas, cv=tscv))\n",
    "])\n",
    "\n",
    "preds = []\n",
    "dates_pred = []\n",
    "r2_is_list = []\n",
    "r2_os_list=[]\n",
    "hit_rate_list = []\n",
    "alpha,selected_features = [],[]\n",
    "\n",
    "preds_nopca = []\n",
    "dates_pred_nopca = []\n",
    "r2_is_list_nopca = []\n",
    "r2_os_list_nopca=[]\n",
    "hit_rate_list_nopca = []\n",
    "alpha_nopca,selected_features_nopca = [],[]\n",
    "\n",
    "for start in tqdm(range(0, len(X) - window_train - window_pred + 1, window_pred)):\n",
    "  \n",
    "    end_train = start + window_train\n",
    "    end_pred = end_train + window_pred\n",
    "\n",
    "    X_train = X.iloc[start:end_train]\n",
    "    Y_train = Y.iloc[start:end_train]\n",
    "\n",
    "    X_test = X.iloc[end_train:end_pred]\n",
    "    Y_test = Y.iloc[end_train:end_pred]\n",
    "\n",
    "\n",
    "    # model with PCA \n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    preds.append(Y_pred)\n",
    "    dates_pred.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred), axis=0)\n",
    "    r2_is_list.append(r2_is)\n",
    "    r2_os_list.append(r2_oos)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    alpha.append(pipe.named_steps['lasso'].alpha_)\n",
    "    selected_features.append(np.sum(np.any(pipe.named_steps['lasso'].coef_!=0,axis=0)))\n",
    "\n",
    "\n",
    "    # model without PCA \n",
    "    pipe_nopca.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_nopca = pipe_nopca.predict(X_test)\n",
    "\n",
    "    preds_nopca.append(Y_pred)\n",
    "    dates_pred_nopca.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe_nopca.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred_nopca), axis=0)\n",
    "    r2_is_list_nopca.append(r2_is)\n",
    "    r2_os_list_nopca.append(r2_oos)\n",
    "    hit_rate_list_nopca.append(hit_rate)\n",
    "    alpha_nopca.append(pipe_nopca.named_steps['lasso'].alpha_)\n",
    "    selected_features_nopca.append(np.sum(np.any(pipe_nopca.named_steps['lasso'].coef_!=0,axis=0)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0852b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_is_df = pd.DataFrame(r2_is_list) \n",
    "r2_is_df_nopca = pd.DataFrame(r2_is_list_nopca) \n",
    "r2_is_df.columns = Y.columns\n",
    "r2_is_df_nopca.columns = Y.columns\n",
    "r2_is_df.index = [date[0] for date in dates_pred]\n",
    "r2_is_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "r2_os_df = pd.DataFrame(r2_os_list)  \n",
    "r2_os_df.columns = Y.columns\n",
    "r2_os_df.index = [date[0] for date in dates_pred]\n",
    "r2_os_df_nopca = pd.DataFrame(r2_os_list_nopca) \n",
    "r2_os_df_nopca.columns = Y.columns\n",
    "r2_os_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "hr = pd.DataFrame(hit_rate_list) \n",
    "hr.columns = Y.columns\n",
    "hr.index = [date[0] for date in dates_pred]\n",
    "hr_nopca = pd.DataFrame(hit_rate_list_nopca) \n",
    "hr_nopca.columns = Y.columns\n",
    "hr_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs = pd.DataFrame(selected_features)\n",
    "fs.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs_nopca = pd.DataFrame(selected_features_nopca)\n",
    "fs_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "alphadf = pd.DataFrame(alpha)\n",
    "alphadf.index = [date[0] for date in dates_pred]\n",
    "\n",
    "alphadf_nopca = pd.DataFrame(alpha_nopca)\n",
    "alphadf_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(5,2,figsize=(28,28))\n",
    "\n",
    "r2_is_df.plot(ax = ax[0,0],title = 'Evolution of in sample R2 per maturity with PCA, per sample')\n",
    "ax[0,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_is_df_nopca.plot(ax = ax[0,1],title = 'Evolution of in sample R2 per maturity without PCA, per sample')\n",
    "ax[0,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "r2_os_df.plot(ax = ax[1,0], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity with PCA, per sample')\n",
    "ax[1,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_os_df_nopca.plot(ax = ax[1,1], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity without PCA, per sample')\n",
    "ax[1,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "hr.plot(ax = ax[2,0], title = 'Evolution of out-sample hit rate per maturity with PCA, per sample')\n",
    "ax[2,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "hr_nopca.plot(ax = ax[2,1], title = 'Evolution of out-sample hit rate per maturity without PCA, per sample')\n",
    "ax[2,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "fs.plot(ax = ax[3,0], title = 'Evolution of number of selected features with PCA, per sample')\n",
    "ax[3,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "fs_nopca.plot(ax = ax[3,1], title = 'Evolution of number of selected features without PCA, per sample')\n",
    "ax[3,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "alphadf.plot(ax = ax[4,0], title = 'Evolution of alpha selected by cross validation - model with PCA, per sample')\n",
    "ax[4,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "alphadf_nopca.plot(ax = ax[4,1], title = 'Evolution of alpha selected by cross validation - model without PCA, per sample')\n",
    "ax[4,1].grid(True, axis='y', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129a4af",
   "metadata": {},
   "source": [
    "Lasso performance is even worse than that of ridge because there are some periods for which the model selects no features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c5160",
   "metadata": {},
   "source": [
    "## Training Elastic net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 252 *10\n",
    "window_pred = 21          \n",
    "alphas = np.logspace(-3, 2, 20)\n",
    "l1_ratios = [0.25,0.5,0.75]\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.95)),   \n",
    "    ('elasticnet', MultiTaskElasticNetCV(alphas=alphas, cv=tscv, l1_ratio=0.5, fit_intercept=False,max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe_nopca = Pipeline([\n",
    "    ('scaler',StandardScaler()),   \n",
    "    ('elasticnet', MultiTaskElasticNetCV(alphas=alphas, cv=tscv, l1_ratio=l1_ratios, fit_intercept=False,max_iter=5000))\n",
    "])\n",
    "\n",
    "preds = []\n",
    "dates_pred = []\n",
    "r2_is_list = []\n",
    "r2_os_list=[]\n",
    "hit_rate_list = []\n",
    "alpha,selected_features,l1 = [],[],[]\n",
    "\n",
    "preds_nopca = []\n",
    "dates_pred_nopca = []\n",
    "r2_is_list_nopca = []\n",
    "r2_os_list_nopca=[]\n",
    "hit_rate_list_nopca = []\n",
    "alpha_nopca,selected_features_nopca,l1_nopca = [],[],[]\n",
    "\n",
    "for start in tqdm(range(0, len(X) - window_train - window_pred + 1, window_pred)):\n",
    "  \n",
    "    end_train = start + window_train\n",
    "    end_pred = end_train + window_pred\n",
    "\n",
    "    X_train = X.iloc[start:end_train]\n",
    "    Y_train = Y.iloc[start:end_train]\n",
    "\n",
    "    X_test = X.iloc[end_train:end_pred]\n",
    "    Y_test = Y.iloc[end_train:end_pred]\n",
    "\n",
    "\n",
    "    # model with PCA \n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    preds.append(Y_pred)\n",
    "    dates_pred.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred), axis=0)\n",
    "    r2_is_list.append(r2_is)\n",
    "    r2_os_list.append(r2_oos)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    alpha.append(pipe.named_steps['elasticnet'].alpha_)\n",
    "    selected_features.append(np.sum(np.any(pipe.named_steps['elasticnet'].coef_!=0,axis=0)))\n",
    "    l1.append(pipe.named_steps['elasticnet'].l1_ratio_)\n",
    "\n",
    "\n",
    "    # model without PCA \n",
    "    pipe_nopca.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_nopca = pipe_nopca.predict(X_test)\n",
    "\n",
    "    preds_nopca.append(Y_pred)\n",
    "    dates_pred_nopca.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe_nopca.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred_nopca), axis=0)\n",
    "    r2_is_list_nopca.append(r2_is)\n",
    "    r2_os_list_nopca.append(r2_oos)\n",
    "    hit_rate_list_nopca.append(hit_rate)\n",
    "    alpha_nopca.append(pipe_nopca.named_steps['elasticnet'].alpha_)\n",
    "    selected_features_nopca.append(np.sum(np.any(pipe_nopca.named_steps['elasticnet'].coef_!=0,axis=0)))\n",
    "    l1_nopca.append(pipe_nopca.named_steps['elasticnet'].l1_ratio_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_is_df = pd.DataFrame(r2_is_list) \n",
    "r2_is_df_nopca = pd.DataFrame(r2_is_list_nopca) \n",
    "r2_is_df.columns = Y.columns\n",
    "r2_is_df_nopca.columns = Y.columns\n",
    "r2_is_df.index = [date[0] for date in dates_pred]\n",
    "r2_is_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "r2_os_df = pd.DataFrame(r2_os_list)  \n",
    "r2_os_df.columns = Y.columns\n",
    "r2_os_df.index = [date[0] for date in dates_pred]\n",
    "r2_os_df_nopca = pd.DataFrame(r2_os_list_nopca) \n",
    "r2_os_df_nopca.columns = Y.columns\n",
    "r2_os_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "hr = pd.DataFrame(hit_rate_list) \n",
    "hr.columns = Y.columns\n",
    "hr.index = [date[0] for date in dates_pred]\n",
    "hr_nopca = pd.DataFrame(hit_rate_list_nopca) \n",
    "hr_nopca.columns = Y.columns\n",
    "hr_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs = pd.DataFrame(selected_features)\n",
    "fs.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs_nopca = pd.DataFrame(selected_features_nopca)\n",
    "fs_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "alphadf = pd.DataFrame(alpha)\n",
    "alphadf.index = [date[0] for date in dates_pred]\n",
    "\n",
    "alphadf_nopca = pd.DataFrame(alpha_nopca)\n",
    "alphadf_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "l1df = pd.DataFrame(l1)\n",
    "l1df.index = [date[0] for date in dates_pred]\n",
    "\n",
    "l1df_nopca = pd.DataFrame(l1_nopca)\n",
    "l1df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(6,2,figsize=(28,28))\n",
    "\n",
    "r2_is_df.plot(ax = ax[0,0],title = 'Evolution of in sample R2 per maturity with PCA, per sample')\n",
    "ax[0,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_is_df_nopca.plot(ax = ax[0,1],title = 'Evolution of in sample R2 per maturity without PCA, per sample')\n",
    "ax[0,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "r2_os_df.plot(ax = ax[1,0], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity with PCA, per sample')\n",
    "ax[1,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_os_df_nopca.plot(ax = ax[1,1], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity without PCA, per sample')\n",
    "ax[1,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "hr.plot(ax = ax[2,0], title = 'Evolution of out-sample hit rate per maturity with PCA, per sample')\n",
    "ax[2,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "hr_nopca.plot(ax = ax[2,1], title = 'Evolution of out-sample hit rate per maturity without PCA, per sample')\n",
    "ax[2,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "fs.plot(ax = ax[3,0], title = 'Evolution of number of selected features with PCA, per sample')\n",
    "ax[3,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "fs_nopca.plot(ax = ax[3,1], title = 'Evolution of number of selected features without PCA, per sample')\n",
    "ax[3,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "alphadf.plot(ax = ax[4,0], title = 'Evolution of alpha selected by cross validation - model with PCA, per sample')\n",
    "ax[4,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "alphadf_nopca.plot(ax = ax[4,1], title = 'Evolution of alpha selected by cross validation - model without PCA, per sample')\n",
    "ax[4,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "l1df.plot(ax = ax[5,0], title = 'Evolution of L1 ratio selected by cross validation - model with PCA, per sample')\n",
    "ax[5,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "l1df_nopca.plot(ax = ax[5,1], title = 'Evolution of L1 ratio selected by cross validation - model without PCA, per sample')\n",
    "ax[5,1].grid(True, axis='y', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09193bf",
   "metadata": {},
   "source": [
    "Overall all these linear models work very poorly, which is not surprising given the low correlation between features and past yields. We can now try to work on weekly data, which may be a bit less noisy: hopefully our model will be able to better identify some trends and patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08c70c",
   "metadata": {},
   "source": [
    "# II) Training linear models on weekly data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join('data', 'US', 'us_data_weekly.csv')\n",
    "dus = pd.read_csv(datapath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe448618",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus = dus[dus.index>'2003-01-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db708b",
   "metadata": {},
   "source": [
    "### A) Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for USyield in ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3','DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30']:\n",
    "    series = dus[USyield]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "    plot_acf(series, lags=50,title = f'ACF {USyield}', ax = axes[0])\n",
    "    plot_pacf(series, lags=50,title = f'PACF {USyield}',ax = axes[1])\n",
    "    \n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0.5, 50)  # décale le début après 0\n",
    "        ax.set_ylim(-0.2, 0.2) \n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd433bc6",
   "metadata": {},
   "source": [
    "- We can see that on short term yields, there is much more autocorrelation in the data, up to 40 lags. Returns in the past few weeks (and so on the past year) are highly correlated to returns in the next week. \n",
    "- However, on long term yields, there is much less autocorrelation and returns in the past 2 weeks are only very slightly correlated to next day return. Surprisingly we see some persistent autocorrelation between returns at day t and t-25 and t-50. \n",
    "\n",
    "For maturities less than 1y, we'll add the following lags:\n",
    "- t-1,t-2,t-5,t-10,t-15,t-20,t-25,t-30,t-40\n",
    "\n",
    "For maturities more than 1y, we will add:\n",
    "- t-1,t-2,t-10\n",
    "\n",
    "We will probably need to do some PCA to combine features as they will be very correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeeec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1,2,5,10,15,20,25,30,40,50]:\n",
    "    dus[f'DGS1MO_t-{lag}'] = dus['DGS1MO'].shift(lag-1)\n",
    "    dus[f'DGS3MO_t-{lag}'] = dus['DGS3MO'].shift(lag-1)\n",
    "    dus[f'DGS6MO_t-{lag}'] = dus['DGS6MO'].shift(lag-1)\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "  \n",
    "\n",
    "for lag in [1,2,10]:\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "    dus[f'DGS2_t-{lag}'] = dus['DGS2'].shift(lag-1)\n",
    "    dus[f'DGS3_t-{lag}'] = dus['DGS3'].shift(lag-1)\n",
    "    dus[f'DGS5_t-{lag}'] = dus['DGS5'].shift(lag-1)\n",
    "    dus[f'DGS7_t-{lag}'] = dus['DGS7'].shift(lag-1)\n",
    "    dus[f'DGS10_t-{lag}'] = dus['DGS10'].shift(lag-1)\n",
    "    dus[f'DGS20_t-{lag}'] = dus['DGS20'].shift(lag-1)\n",
    "    dus[f'DGS30_t-{lag}'] = dus['DGS30'].shift(lag-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables to forecast \n",
    "\n",
    "dus['Y_1MO'] = dus['DGS1MO'].shift(-1)\n",
    "dus['Y_3MO'] = dus['DGS3MO'].shift(-1)\n",
    "dus['Y_6MO'] = dus['DGS6MO'].shift(-1)\n",
    "dus['Y_1year'] = dus['DGS1'].shift(-1)\n",
    "dus['Y_2year'] = dus['DGS2'].shift(-1)\n",
    "dus['Y_3year'] = dus['DGS3'].shift(-1)\n",
    "dus['Y_5year'] = dus['DGS5'].shift(-1)\n",
    "dus['Y_7year'] = dus['DGS7'].shift(-1)\n",
    "dus['Y_10year'] = dus['DGS10'].shift(-1)\n",
    "dus['Y_20year'] = dus['DGS20'].shift(-1)\n",
    "dus['Y_30year'] = dus['DGS30'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be9808",
   "metadata": {},
   "source": [
    "We'll also add statistical features like the mean, variance, autocorrelation, quantiles of the time series to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ts_features(df, cols, max_lag=30, windows=[20, 60]):\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for col in cols:\n",
    "        y = df[col]\n",
    "\n",
    "  \n",
    "        # --- Statistiques glissantes ---\n",
    "        for w in windows:\n",
    "            features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
    "            features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
    "            features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
    "            features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
    "            features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
    "            features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
    "            features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min() \n",
    "        \n",
    "       \n",
    "        # --- Autocorrélations locales ---\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            features[f'{col}_autocorr_{lag}'] = (\n",
    "                y.rolling(window=max(windows)).apply(lambda x: x.autocorr(lag=lag), raw=False)\n",
    "            )\n",
    "        \n",
    "    return features\n",
    "\n",
    "# Exemple d’usage :\n",
    "cols = ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3',\n",
    "        'DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30']\n",
    "\n",
    "dus_features = add_ts_features(dus, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae323cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus = dus.merge(dus_features, how = 'inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus = dus.dropna()\n",
    "dus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now remove the original yield columns\n",
    "dus = dus.drop(columns=['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3','DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6d6c4",
   "metadata": {},
   "source": [
    "Many features aren't that much correlated to the target, so we'll remove all features with correlation lower than 0.1 to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dus[['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']]\n",
    "\n",
    "corrs = pd.DataFrame({\n",
    "    target: dus.corrwith(Y[target]) for target in Y.columns\n",
    "}).abs()  \n",
    "\n",
    "# repérer les colonnes où la corrélation absolue < 0.05 pour toutes les targets\n",
    "mask = (corrs < 0.1).all(axis=1)\n",
    "low_corr_features = corrs.index[mask]\n",
    "\n",
    "# supprimer ces colonnes\n",
    "dus_filtered = dus.drop(columns=low_corr_features)\n",
    "\n",
    "print(f\"{len(low_corr_features)} features were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a959c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c80cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,25))\n",
    "sns.heatmap(dus_filtered.corr(), cmap='seismic', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dus_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9ecb7",
   "metadata": {},
   "source": [
    "### B) Training a ridge model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259923a",
   "metadata": {},
   "source": [
    "We can now try to train a ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dus_filtered[['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']]\n",
    "\n",
    "X = dus_filtered.drop(columns = ['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316231bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 52*5 #we train our model on 5 years of data and test it on the next month \n",
    "window_pred = 4          \n",
    "alphas = np.logspace(-1, 3, 20)\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.95)),   \n",
    "    ('ridge', MultiOutputRegressor(RidgeCV(fit_intercept=False,alphas=alphas, cv=tscv)))\n",
    "])\n",
    "\n",
    "pipe_nopca = Pipeline([\n",
    "    ('scaler',StandardScaler()),   \n",
    "    ('ridge', MultiOutputRegressor(RidgeCV(fit_intercept=False,alphas=alphas, cv=tscv)))\n",
    "])\n",
    "\n",
    "preds = []\n",
    "dates_pred = []\n",
    "r2_is_list = []\n",
    "r2_os_list=[]\n",
    "hit_rate_list = []\n",
    "\n",
    "preds_nopca = []\n",
    "dates_pred_nopca = []\n",
    "r2_is_list_nopca = []\n",
    "r2_os_list_nopca=[]\n",
    "hit_rate_list_nopca = []\n",
    "\n",
    "for start in tqdm(range(0, len(X) - window_train - window_pred + 1, window_pred)):\n",
    "  \n",
    "    end_train = start + window_train\n",
    "    end_pred = end_train + window_pred\n",
    "\n",
    "    X_train = X.iloc[start:end_train]\n",
    "    Y_train = Y.iloc[start:end_train]\n",
    "\n",
    "    X_test = X.iloc[end_train:end_pred]\n",
    "    Y_test = Y.iloc[end_train:end_pred]\n",
    "\n",
    "\n",
    "    # model with PCA \n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    preds.append(Y_pred)\n",
    "    dates_pred.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred), axis=0)\n",
    "    r2_is_list.append(r2_is)\n",
    "    r2_os_list.append(r2_oos)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "\n",
    "\n",
    "    # model without PCA \n",
    "    pipe_nopca.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_nopca = pipe_nopca.predict(X_test)\n",
    "\n",
    "    preds_nopca.append(Y_pred)\n",
    "    dates_pred_nopca.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe_nopca.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred_nopca), axis=0)\n",
    "    r2_is_list_nopca.append(r2_is)\n",
    "    r2_os_list_nopca.append(r2_oos)\n",
    "    hit_rate_list_nopca.append(hit_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_is_df = pd.DataFrame(r2_is_list) \n",
    "r2_is_df_nopca = pd.DataFrame(r2_is_list_nopca) \n",
    "r2_is_df.columns = Y.columns\n",
    "r2_is_df_nopca.columns = Y.columns\n",
    "r2_is_df.index = [date[0] for date in dates_pred]\n",
    "r2_is_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "r2_os_df = pd.DataFrame(r2_os_list)  \n",
    "r2_os_df.columns = Y.columns\n",
    "r2_os_df.index = [date[0] for date in dates_pred]\n",
    "r2_os_df_nopca = pd.DataFrame(r2_os_list_nopca) \n",
    "r2_os_df_nopca.columns = Y.columns\n",
    "r2_os_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "hr = pd.DataFrame(hit_rate_list) \n",
    "hr.columns = Y.columns\n",
    "hr.index = [date[0] for date in dates_pred]\n",
    "hr_nopca = pd.DataFrame(hit_rate_list_nopca) \n",
    "hr_nopca.columns = Y.columns\n",
    "hr_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "plt.close('all')  # ferme toutes les figures existantes\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,2,figsize=(28,28))\n",
    "\n",
    "r2_is_df.plot(ax = ax[0,0],title = 'Evolution of in sample R2 per model with PCA, per sample')\n",
    "ax[0,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_is_df_nopca.plot(ax = ax[0,1],title = 'Evolution of in sample R2 per model without PCA, per sample')\n",
    "ax[0,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "r2_os_df.plot(ax = ax[1,0], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per model with PCA, per sample')\n",
    "ax[1,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_os_df_nopca.plot(ax = ax[1,1], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per model without PCA, per sample')\n",
    "ax[1,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "hr.plot(ax = ax[2,0], title = 'Evolution of out-sample hit rate per model with PCA, per sample')\n",
    "ax[2,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "hr_nopca.plot(ax = ax[2,1], title = 'Evolution of out-sample hit rate per model without PCA, per sample')\n",
    "ax[2,1].grid(True, axis='y', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab5e46",
   "metadata": {},
   "source": [
    "Results are slightly better than when working on daily data, but we still only learn noise and completely overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83421aee",
   "metadata": {},
   "source": [
    "### B) Training an elastic net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 52*5\n",
    "window_pred = 4         \n",
    "alphas = np.logspace(-2, 2, 20)\n",
    "l1_ratios = [0.25,0.5,0.75]\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.95)),   \n",
    "    ('elasticnet', MultiTaskElasticNetCV(alphas=alphas, cv=tscv, l1_ratio=0.5, fit_intercept=False,max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe_nopca = Pipeline([\n",
    "    ('scaler',StandardScaler()),   \n",
    "    ('elasticnet', MultiTaskElasticNetCV(alphas=alphas, cv=tscv, l1_ratio=l1_ratios, fit_intercept=False,max_iter=5000))\n",
    "])\n",
    "\n",
    "preds = []\n",
    "dates_pred = []\n",
    "r2_is_list = []\n",
    "r2_os_list=[]\n",
    "hit_rate_list = []\n",
    "alpha,selected_features,l1 = [],[],[]\n",
    "\n",
    "preds_nopca = []\n",
    "dates_pred_nopca = []\n",
    "r2_is_list_nopca = []\n",
    "r2_os_list_nopca=[]\n",
    "hit_rate_list_nopca = []\n",
    "alpha_nopca,selected_features_nopca,l1_nopca = [],[],[]\n",
    "\n",
    "for start in tqdm(range(0, len(X) - window_train - window_pred + 1, window_pred)):\n",
    "  \n",
    "    end_train = start + window_train\n",
    "    end_pred = end_train + window_pred\n",
    "\n",
    "    X_train = X.iloc[start:end_train]\n",
    "    Y_train = Y.iloc[start:end_train]\n",
    "\n",
    "    X_test = X.iloc[end_train:end_pred]\n",
    "    Y_test = Y.iloc[end_train:end_pred]\n",
    "\n",
    "\n",
    "    # model with PCA \n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    preds.append(Y_pred)\n",
    "    dates_pred.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred), axis=0)\n",
    "    r2_is_list.append(r2_is)\n",
    "    r2_os_list.append(r2_oos)\n",
    "    hit_rate_list.append(hit_rate)\n",
    "    alpha.append(pipe.named_steps['elasticnet'].alpha_)\n",
    "    selected_features.append(np.sum(np.any(pipe.named_steps['elasticnet'].coef_!=0,axis=0)))\n",
    "    l1.append(pipe.named_steps['elasticnet'].l1_ratio_)\n",
    "\n",
    "\n",
    "    # model without PCA \n",
    "    pipe_nopca.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_nopca = pipe_nopca.predict(X_test)\n",
    "\n",
    "    preds_nopca.append(Y_pred)\n",
    "    dates_pred_nopca.append(X.index[end_train:end_pred])\n",
    "\n",
    "    mse = mean_squared_error(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    r2_is = r2_score(Y_train, pipe_nopca.predict(X_train), multioutput='raw_values')\n",
    "    r2_oos = r2_score(Y_test, Y_pred_nopca, multioutput='raw_values')\n",
    "    hit_rate = np.mean(np.sign(Y_test.values) == np.sign(Y_pred_nopca), axis=0)\n",
    "    r2_is_list_nopca.append(r2_is)\n",
    "    r2_os_list_nopca.append(r2_oos)\n",
    "    hit_rate_list_nopca.append(hit_rate)\n",
    "    alpha_nopca.append(pipe_nopca.named_steps['elasticnet'].alpha_)\n",
    "    selected_features_nopca.append(np.sum(np.any(pipe_nopca.named_steps['elasticnet'].coef_!=0,axis=0)))\n",
    "    l1_nopca.append(pipe_nopca.named_steps['elasticnet'].l1_ratio_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_is_df = pd.DataFrame(r2_is_list) \n",
    "r2_is_df_nopca = pd.DataFrame(r2_is_list_nopca) \n",
    "r2_is_df.columns = Y.columns\n",
    "r2_is_df_nopca.columns = Y.columns\n",
    "r2_is_df.index = [date[0] for date in dates_pred]\n",
    "r2_is_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "r2_os_df = pd.DataFrame(r2_os_list)  \n",
    "r2_os_df.columns = Y.columns\n",
    "r2_os_df.index = [date[0] for date in dates_pred]\n",
    "r2_os_df_nopca = pd.DataFrame(r2_os_list_nopca) \n",
    "r2_os_df_nopca.columns = Y.columns\n",
    "r2_os_df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "hr = pd.DataFrame(hit_rate_list) \n",
    "hr.columns = Y.columns\n",
    "hr.index = [date[0] for date in dates_pred]\n",
    "hr_nopca = pd.DataFrame(hit_rate_list_nopca) \n",
    "hr_nopca.columns = Y.columns\n",
    "hr_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs = pd.DataFrame(selected_features)\n",
    "fs.index = [date[0] for date in dates_pred]\n",
    "\n",
    "fs_nopca = pd.DataFrame(selected_features_nopca)\n",
    "fs_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "alphadf = pd.DataFrame(alpha)\n",
    "alphadf.index = [date[0] for date in dates_pred]\n",
    "\n",
    "alphadf_nopca = pd.DataFrame(alpha_nopca)\n",
    "alphadf_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "l1df = pd.DataFrame(l1)\n",
    "l1df.index = [date[0] for date in dates_pred]\n",
    "\n",
    "l1df_nopca = pd.DataFrame(l1_nopca)\n",
    "l1df_nopca.index = [date[0] for date in dates_pred]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(6,2,figsize=(28,28))\n",
    "\n",
    "r2_is_df.plot(ax = ax[0,0],title = 'Evolution of in sample R2 per maturity with PCA, per sample')\n",
    "ax[0,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_is_df_nopca.plot(ax = ax[0,1],title = 'Evolution of in sample R2 per maturity without PCA, per sample')\n",
    "ax[0,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "r2_os_df.plot(ax = ax[1,0], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity with PCA, per sample')\n",
    "ax[1,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "r2_os_df_nopca.plot(ax = ax[1,1], ylim =(-0.3,0.3), title = 'Evolution of out-sample R2 per maturity without PCA, per sample')\n",
    "ax[1,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "hr.plot(ax = ax[2,0], title = 'Evolution of out-sample hit rate per maturity with PCA, per sample')\n",
    "ax[2,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "hr_nopca.plot(ax = ax[2,1], title = 'Evolution of out-sample hit rate per maturity without PCA, per sample')\n",
    "ax[2,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "fs.plot(ax = ax[3,0], title = 'Evolution of number of selected features with PCA, per sample')\n",
    "ax[3,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "fs_nopca.plot(ax = ax[3,1], title = 'Evolution of number of selected features without PCA, per sample')\n",
    "ax[3,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "alphadf.plot(ax = ax[4,0], title = 'Evolution of alpha selected by cross validation - model with PCA, per sample')\n",
    "ax[4,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "alphadf_nopca.plot(ax = ax[4,1], title = 'Evolution of alpha selected by cross validation - model without PCA, per sample')\n",
    "ax[4,1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "l1df.plot(ax = ax[5,0], title = 'Evolution of L1 ratio selected by cross validation - model with PCA, per sample')\n",
    "ax[5,0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "l1df_nopca.plot(ax = ax[5,1], title = 'Evolution of L1 ratio selected by cross validation - model without PCA, per sample')\n",
    "ax[5,1].grid(True, axis='y', linestyle='--', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27653ef",
   "metadata": {},
   "source": [
    "The results are also disappointing. Overall this is not surprising, we've seen that our features have a low correlation with the yield so the predictive power of these models is logically very low. We'll now try to do binary classification, maybe it could work better. Moreover, we'll focus on more complex models to see if they can extract non linear relationships between features and yields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa254c",
   "metadata": {},
   "source": [
    "# III - Binary classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f1e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join('data', 'US', 'us_data_weekly.csv')\n",
    "dus_weekly = pd.read_csv(datapath, index_col=0)\n",
    "dus_weekly = dus_weekly.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafec19",
   "metadata": {},
   "source": [
    "### A) Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc344f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_ts_features(df, cols, max_lag=20, windows=[10,20,50]):\n",
    "\n",
    "    all_features = {}\n",
    "\n",
    "    for col in cols:\n",
    "        y = df[col]\n",
    "\n",
    "        # --- Rolling statistics ---\n",
    "        for w in windows:\n",
    "            roll = y.rolling(w)\n",
    "            all_features[f'{col}_mean_{w}'] = roll.mean()\n",
    "            all_features[f'{col}_std_{w}'] = roll.std()\n",
    "            all_features[f'{col}_q25_{w}'] = roll.quantile(0.25)\n",
    "            all_features[f'{col}_q75_{w}'] = roll.quantile(0.75)\n",
    "            all_features[f'{col}_q05_{w}'] = roll.quantile(0.05)\n",
    "            all_features[f'{col}_q90_{w}'] = roll.quantile(0.9)\n",
    "            all_features[f'{col}_range_{w}'] = roll.max() - roll.min()\n",
    "\n",
    "            # Z-score et momentum\n",
    "            all_features[f'{col}_zscore_{w}'] = (y - roll.mean()) / roll.std()\n",
    "            all_features[f'{col}_momentum_{w}'] = y - y.shift(w)\n",
    "\n",
    "        # Ratio de moyennes rapides / lentes\n",
    "        all_features[f'{col}_ratio_{10}_{50}'] = (\n",
    "            y.rolling(10).mean() / y.rolling(50).mean()\n",
    "        )\n",
    "\n",
    "        # Volatilité annualisée approx\n",
    "        all_features[f'{col}_vol_20'] = y.rolling(20).std() * np.sqrt(52)\n",
    "\n",
    "        # --- Lags bruts ---\n",
    "        for lag in [1,2,3,4,5,10,15,20,25,30,40,50]:\n",
    "            all_features[f'{col}_lag_{lag}'] = y.shift(lag-1)\n",
    "\n",
    "        # --- Autocorrélations (in-sample) ---\n",
    "        \n",
    "        acf_vals = acf(y.dropna(), nlags=max_lag, fft=True)\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            all_features[f'{col}_autocorr_{lag}'] = acf_vals[lag]\n",
    "\n",
    "    # --- Construction finale ---\n",
    "    features = pd.DataFrame(all_features, index=df.index)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58251785",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3',\n",
    "        'DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30']\n",
    "\n",
    "dus_features = add_ts_features(dus_weekly, cols)\n",
    "dus_features= dus_features.dropna()\n",
    "dus_weekly = dus_weekly.merge(dus_features, how = 'inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890a9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols: \n",
    "    dus_weekly[f'Y_{col}'] = (dus_weekly[col]>0).astype(int).shift(-1)\n",
    "\n",
    "\n",
    "dus_weekly= dus_weekly.dropna()\n",
    "dus_weekly = dus_weekly.replace([np.inf, -np.inf], np.nan)\n",
    "dus_weekly = dus_weekly.ffill()\n",
    "\n",
    "#we can now remove the original yield columns\n",
    "dus_weekly = dus_weekly.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9a2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 738 features in the dataset\n"
     ]
    }
   ],
   "source": [
    "Yw = dus_weekly[[f'Y_{col}' for col in cols]]\n",
    "Xw = dus_weekly.drop(columns = [f'Y_{col}' for col in cols])\n",
    "\n",
    "print(f\"there are {Xw.shape[1]} features in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f3a58",
   "metadata": {},
   "source": [
    "We augmented the dataset by adding a very large amount of features created from the yields time series. however, many of these features are not informative so we need to remove them before training our model. \n",
    "\n",
    "We'll filter features by keeping only those with a mutual information score above than a given threshold. We'll create several datasets of features containing features filtered for the thresholds 0.03, 0.035, 0.04, 0.05 and train models on these specific datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a09ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:42<02:07, 42.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we removed 464 features in the weekly dataset for threshold 0.03\n",
      "Number of variables in the dataset with MI threshold 0.03: 274\n",
      "Macro and market variables in the dataset with MI threshold 0.03: ['UNRATE', 'USGOVT', 'SRVPRD', 'MANEMP', 'USTPU', 'M2SL', 'USCONS', 'PERMIT', 'IRLTLT01FRM156N', 'CURRCIR', 'GFDEGDQ188S', 'DNDGRG3M086SBEA', 'IRLTLT01DEM156N', 'CPITRNSL', 'DTCTHFNM', 'FGDEF', 'IRLTLT01JPM156N', 'IRLTLT01AUM156N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [01:34<01:35, 47.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we removed 559 features in the weekly dataset for threshold 0.035\n",
      "Number of variables in the dataset with MI threshold 0.035: 179\n",
      "Macro and market variables in the dataset with MI threshold 0.035: ['USTPU', 'TOTRESNS', 'UEMP5TO14', 'USCONS', 'IRLTLT01CAM156N', 'IRLTLT01DEM156N', 'IRLTLT01AUM156N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [02:16<00:45, 45.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we removed 639 features in the weekly dataset for threshold 0.04\n",
      "Number of variables in the dataset with MI threshold 0.04: 99\n",
      "Macro and market variables in the dataset with MI threshold 0.04: ['SRVPRD', 'MTSDS133FMS', 'USTPU', 'GFDEGDQ188S', 'IRLTLT01DEM156N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:53<00:00, 43.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we removed 698 features in the weekly dataset for threshold 0.05\n",
      "Number of variables in the dataset with MI threshold 0.05: 40\n",
      "Macro and market variables in the dataset with MI threshold 0.05: ['MTSDS133FMS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "initial_number_of_features = Xw.shape[1]\n",
    "\n",
    "threshold_list = [0.03,0.035,0.04,0.05]\n",
    "\n",
    "datasets = {t:pd.DataFrame() for t in threshold_list}\n",
    "\n",
    "for threshold_mi in tqdm(threshold_list): \n",
    "    selected_features_w = set()\n",
    "    for col in Yw.columns:\n",
    "        mi = mutual_info_classif(Xw, Yw[col])\n",
    "        top_features_i = Xw.columns[mi > threshold_mi]  \n",
    "        selected_features_w.update(top_features_i)\n",
    "\n",
    "    datasets[threshold_mi] = Xw[list(selected_features_w)]\n",
    "\n",
    "    print(f'we removed {initial_number_of_features-len(selected_features_w)} features in the weekly dataset for threshold {threshold_mi}')\n",
    "    print(f'Number of variables in the dataset with MI threshold {threshold_mi}:',datasets[threshold_mi].shape[1])\n",
    "    print(f'Macro and market variables in the dataset with MI threshold {threshold_mi}:', [col for col in datasets[threshold_mi].columns if 'DGS' not in col])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4c612",
   "metadata": {},
   "source": [
    "Notice that many of the macro and market variables that we extracted from the FRED website do not share much mutual information with the target since many of these variables are removed with the threshold we used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819007b",
   "metadata": {},
   "source": [
    "### B) Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c619595",
   "metadata": {},
   "source": [
    "We define the following pipelines for our logistic regression models.\n",
    "We train our model on a rolling window of 15 years of data (with cross validation without leakage of future information) and then predict the 4 next weeks. We then update the rolling window, train another model, predict the 4 next weekds and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c291d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 52 * 15\n",
    "window_pred = 4\n",
    "\n",
    "alphas = np.logspace(-5, 2, 30)\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', MultiOutputClassifier(\n",
    "        LogisticRegressionCV(\n",
    "            penalty='l2',\n",
    "            cv=tscv,\n",
    "            Cs=alphas,\n",
    "            fit_intercept=False,\n",
    "            scoring='accuracy',\n",
    "            max_iter=5000)))\n",
    "])\n",
    "\n",
    "pipepca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.99)), \n",
    "    ('logreg', MultiOutputClassifier(\n",
    "        LogisticRegressionCV(\n",
    "            penalty='l2',\n",
    "            cv=tscv,\n",
    "            Cs=alphas,\n",
    "            fit_intercept=False,\n",
    "            scoring='accuracy',\n",
    "            max_iter=5000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0e368",
   "metadata": {},
   "source": [
    "We train logistic regression models for the 4 datasets containing features with mutual information larger than 0.03, 0.035, 0.04, 0.05 respectively. Then we save the results in datasets. \n",
    "\n",
    "The dataset with variables with mutual information > 0.03 contains 266 features, which is very high compared to the numer of points in the training datasets (52*15 lines), so we'll also train a model that performs a PCA before applying the logistic regression in order to reduce the number of features and see if it can improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model on dataset with features with mutual information above 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [35:56<00:00, 27.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > 0.03 is:\n",
      "Y_DGS1MO    0.607595\n",
      "Y_DGS3MO    0.661392\n",
      "Y_DGS6MO    0.636076\n",
      "Y_DGS1      0.591772\n",
      "Y_DGS2      0.582278\n",
      "Y_DGS3      0.563291\n",
      "Y_DGS5      0.525316\n",
      "Y_DGS7      0.496835\n",
      "Y_DGS10     0.487342\n",
      "Y_DGS20     0.509494\n",
      "Y_DGS30     0.515823\n",
      "dtype: float64\n",
      "For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > 0.03 and PCA is:\n",
      "Y_DGS1MO    0.620253\n",
      "Y_DGS3MO    0.667722\n",
      "Y_DGS6MO    0.651899\n",
      "Y_DGS1      0.588608\n",
      "Y_DGS2      0.579114\n",
      "Y_DGS3      0.537975\n",
      "Y_DGS5      0.534810\n",
      "Y_DGS7      0.537975\n",
      "Y_DGS10     0.496835\n",
      "Y_DGS20     0.512658\n",
      "Y_DGS30     0.490506\n",
      "dtype: float64\n",
      "training model on dataset with features with mutual information above 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [24:58<00:00, 18.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > 0.035 is:\n",
      "Y_DGS1MO    0.613924\n",
      "Y_DGS3MO    0.702532\n",
      "Y_DGS6MO    0.645570\n",
      "Y_DGS1      0.544304\n",
      "Y_DGS2      0.515823\n",
      "Y_DGS3      0.556962\n",
      "Y_DGS5      0.509494\n",
      "Y_DGS7      0.474684\n",
      "Y_DGS10     0.443038\n",
      "Y_DGS20     0.487342\n",
      "Y_DGS30     0.465190\n",
      "dtype: float64\n",
      "training model on dataset with features with mutual information above 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [17:00<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > 0.04 is:\n",
      "Y_DGS1MO    0.636076\n",
      "Y_DGS3MO    0.702532\n",
      "Y_DGS6MO    0.686709\n",
      "Y_DGS1      0.598101\n",
      "Y_DGS2      0.525316\n",
      "Y_DGS3      0.518987\n",
      "Y_DGS5      0.525316\n",
      "Y_DGS7      0.477848\n",
      "Y_DGS10     0.493671\n",
      "Y_DGS20     0.471519\n",
      "Y_DGS30     0.474684\n",
      "dtype: float64\n",
      "training model on dataset with features with mutual information above 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [07:56<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > 0.05 is:\n",
      "Y_DGS1MO    0.645570\n",
      "Y_DGS3MO    0.686709\n",
      "Y_DGS6MO    0.629747\n",
      "Y_DGS1      0.598101\n",
      "Y_DGS2      0.550633\n",
      "Y_DGS3      0.531646\n",
      "Y_DGS5      0.512658\n",
      "Y_DGS7      0.471519\n",
      "Y_DGS10     0.484177\n",
      "Y_DGS20     0.458861\n",
      "Y_DGS30     0.471519\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for threshold in threshold_list:\n",
    "\n",
    "    print(f'training model on dataset with features with mutual information above {threshold}')\n",
    "\n",
    "    Xw = datasets[threshold]\n",
    "\n",
    "    # for the dataset containing features with mutual information above 0.03:\n",
    "    # we'll train our logistic model with and without PCA since there are many features in this dataset\n",
    "    if threshold == 0.03:\n",
    "\n",
    "\n",
    "        accuraciesl2 = {col: [] for col in Yw.columns}\n",
    "        accuraciesl2pca = {col: [] for col in Yw.columns}\n",
    "        alphal2 = {col: [] for col in Yw.columns}\n",
    "        alphal2pca = {col: [] for col in Yw.columns}\n",
    "        feature_importancel2 = {col: [] for col in Yw.columns}\n",
    "        feature_importancel2pca = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_predl2 = []\n",
    "        y_predl2pca = []\n",
    "\n",
    "\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "        \n",
    "            pipe.fit(Xw_train, Yw_train)\n",
    "            pipepca.fit(Xw_train, Yw_train)\n",
    "\n",
    "\n",
    "            Yw_pred = pipe.predict(Xw_test)\n",
    "            Yw_predpca = pipepca.predict(Xw_test)\n",
    "\n",
    "            y_predl2.append(Yw_pred)\n",
    "            y_predl2pca.append(Yw_predpca)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuraciesl2[col].append(acc)\n",
    "\n",
    "                acc = accuracy_score(Yw_test[col], Yw_predpca[:, i])\n",
    "                accuraciesl2pca[col].append(acc)\n",
    "\n",
    "                # parameters chosen\n",
    "                best_C = pipe.named_steps['logreg'].estimators_[i].C_[0]\n",
    "                alphal2[col].append(best_C)\n",
    "\n",
    "                best_C = pipepca.named_steps['logreg'].estimators_[i].C_[0]\n",
    "                alphal2pca[col].append(best_C)\n",
    "\n",
    "                # feature importance\n",
    "                coefs = np.abs(pipe.named_steps['logreg'].estimators_[i].coef_).flatten()\n",
    "                feature_importancel2[col].append(coefs)\n",
    "\n",
    "                coefs = np.abs(pipepca.named_steps['logreg'].estimators_[i].coef_).flatten()\n",
    "                feature_importancel2pca[col].append(coefs)\n",
    "\n",
    "        ### saving the results\n",
    "        acc = pd.DataFrame(accuraciesl2,columns = Yw.columns)\n",
    "        params = pd.DataFrame(alphal2, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importancel2, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_predl2).reshape(-1, np.array(y_predl2).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies logistic regression, 15y train test.csv')\n",
    "        params.to_csv('results of models/params logistic regression, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance logistic regression, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast logistic regression, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values logistic regression, 15y train test.csv')\n",
    "\n",
    "        acc = pd.DataFrame(accuraciesl2pca,columns = Yw.columns)\n",
    "        params = pd.DataFrame(alphal2pca, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importancel2pca, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_predl2pca).reshape(-1, np.array(y_predl2pca).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} and PCA is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies logistic regression pca, 15y train test.csv')\n",
    "        params.to_csv('results of models/params logistic regression pca, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance logistic regression pca, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast logistic regression pca, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values logistic regression pca, 15y train test.csv')\n",
    "\n",
    "    #for other datasets, we train models without applying a PCA before. \n",
    "    else: \n",
    "\n",
    "\n",
    "        accuraciesl2 = {col: [] for col in Yw.columns}\n",
    "        alphal2 = {col: [] for col in Yw.columns}\n",
    "        feature_importancel2 = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_predl2 = []\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            pipe.fit(Xw_train, Yw_train)\n",
    "\n",
    "            Yw_pred = pipe.predict(Xw_test)\n",
    "\n",
    "            y_predl2.append(Yw_pred)\n",
    "\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuraciesl2[col].append(acc)\n",
    "\n",
    "\n",
    "                # alpha optimal choisi\n",
    "                best_C = pipe.named_steps['logreg'].estimators_[i].C_[0]\n",
    "                alphal2[col].append(best_C)\n",
    "\n",
    "                # importance des features = moyenne absolue des coefficients\n",
    "                coefs = np.abs(pipe.named_steps['logreg'].estimators_[i].coef_).flatten()\n",
    "                feature_importancel2[col].append(coefs)\n",
    "\n",
    "        acc = pd.DataFrame(accuraciesl2,columns = Yw.columns)\n",
    "        params = pd.DataFrame(alphal2, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importancel2, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_predl2).reshape(-1, np.array(y_predl2).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv(f'results of models/accuracies logistic regression, MI {threshold}, 15y train test.csv')\n",
    "        params.to_csv(f'results of models/params logistic regression, MI {threshold}, 15y train test.csv')\n",
    "        feature_imp.to_csv(f'results of models/feature importance logistic regression, MI {threshold}, 15y train test.csv')\n",
    "        ypred.to_csv(f'results of models/forecast logistic regression, MI {threshold}, 15y train test.csv')\n",
    "        ytrue.to_csv(f'results of models/true values logistic regression, MI {threshold}, 15y train test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b29dd",
   "metadata": {},
   "source": [
    "## C) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9a75c",
   "metadata": {},
   "source": [
    "We'll now train XGboost models to see whether it can capture non linearities in the data and improve accuracy of out-of-sample predictions. Once again, we'll train some models on the 4 datasets that we created. \n",
    "\n",
    "For the first dataset (with features with a MI>0.03), we'll also train a model that performs a PCA to reduce the number of features before applying XGboost. \n",
    "\n",
    "The pipelines that we'll use are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2777039",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 52 * 15\n",
    "window_pred = 4\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "\n",
    "# parameters to test in cross validation \n",
    "param_grid = {\n",
    " 'xgb__estimator__n_estimators': [100, 200],\n",
    " 'xgb__estimator__learning_rate': [0.01, 0.05],\n",
    " 'xgb__estimator__max_depth': [4,7],\n",
    " 'xgb__estimator__subsample': [0.5, 0.7],\n",
    " 'xgb__estimator__colsample_bytree': [0.4, 0.8],\n",
    " 'xgb__estimator__min_child_weight': [5, 10],\n",
    " 'xgb__estimator__reg_alpha': [0.5, 1.0],\n",
    " 'xgb__estimator__reg_lambda': [5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pipeline without PCA \n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "#pipeline with PCA \n",
    "pipepca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca',PCA(n_components = 0.99)),\n",
    "    ('xgb', MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "#and we'll use gridsearchCV to cross validate the model:\n",
    "GridSearchCV(pipepca, #or pipe\n",
    "            param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ace69",
   "metadata": {},
   "source": [
    "We now train XGBoost models on the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e61701",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in threshold_list:\n",
    "\n",
    "    print(f'training model on dataset with features with mutual information above {threshold}')\n",
    "\n",
    "    Xw = datasets[threshold]\n",
    "\n",
    "    # for the dataset containing features with mutual information above 0.03:\n",
    "    # we'll train our xgboost model with and without PCA since there are many features in this dataset\n",
    "    if threshold == 0.03:\n",
    "\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipe,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "                print(col, np.mean(accuracies[col]))\n",
    "\n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['xgb'].estimators_[i]   # XGBClassifier\n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "\n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies xgboost, 15y train test.csv')\n",
    "        params.to_csv('results of models/params xgboost, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance xgboost, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast xgboost, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values xgboost, 15y train test.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #### Now we do the same training but this time we add the PCA in the pipeline before training the model\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "\n",
    "        \n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipepca,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "                print(col, np.mean(accuracies[col]))\n",
    "\n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['xgb'].estimators_[i]   # XGBClassifier\n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "\n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} and PCA is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies xgboost, pca, 15y train test.csv')\n",
    "        params.to_csv('results of models/params xgboost, pca, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance xgboost, pca, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast xgboost, pca, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values xgboost, pca, 15y train test.csv')\n",
    "    \n",
    "\n",
    "    # for other datasets with larger mutual information threshold, we don't apply PCA and train the model directly \n",
    "    else:\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipe,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "                print(col, np.mean(accuracies[col]))\n",
    "\n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['xgb'].estimators_[i]   # XGBClassifier\n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "        \n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv(f'results of models/accuracies xgboost, MI {threshold}, 15y train test.csv')\n",
    "        params.to_csv(f'results of models/params xgboost, MI {threshold}, 15y train test.csv')\n",
    "        feature_imp.to_csv(f'results of models/feature importance xgboost, MI {threshold}, 15y train test.csv')\n",
    "        ypred.to_csv(f'results of models/forecast xgboost, MI {threshold}, 15y train test.csv')\n",
    "        ytrue.to_csv(f'results of models/true values xgboost, MI {threshold}, 15y train test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02f0b8",
   "metadata": {},
   "source": [
    "## D) Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea97c1",
   "metadata": {},
   "source": [
    "We'll proceed as for the logistic regression and XGBoost models when training random forests. The pipelines (with and without PCA) are defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb4ee62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_train = 52 * 15\n",
    "window_pred = 4\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'rf__estimator__n_estimators': [150],\n",
    "    'rf__estimator__max_depth': [4,7],\n",
    "    'rf__estimator__min_samples_leaf': [5,10]\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state = 42\n",
    "            )))\n",
    "])\n",
    "\n",
    "pipepca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca',PCA(n_components=0.99)),\n",
    "    ('rf', MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state = 42\n",
    "            )))\n",
    "])\n",
    "\n",
    "\n",
    "#we will cross validate the model the following way: \n",
    "grid = GridSearchCV(pipe,  #or pipepca\n",
    "                        param_grid,\n",
    "                        cv = tscv,\n",
    "                        scoring = 'accuracy',\n",
    "                        n_jobs=-1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in threshold_list:\n",
    "\n",
    "    print(f'training model on dataset with features with mutual information above {threshold}')\n",
    "\n",
    "    Xw = datasets[threshold]\n",
    "\n",
    "    # for the dataset containing features with mutual information above 0.03:\n",
    "    # we'll train our random forest model with and without PCA since there are many features in this dataset\n",
    "    if threshold == 0.03:\n",
    "\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipe,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "           \n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['rf'].estimators_[i]  \n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "\n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies random forest, 15y train test.csv')\n",
    "        params.to_csv('results of models/params random forest, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance random forest, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast random forest, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values random forest, 15y train test.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #### Now we do the same training but this time we add the PCA in the pipeline before training the model\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "\n",
    "        \n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipepca,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "\n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['rf'].estimators_[i]   \n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "\n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} and PCA is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv('results of models/accuracies random forest, pca, 15y train test.csv')\n",
    "        params.to_csv('results of models/params random forest, pca, 15y train test.csv')\n",
    "        feature_imp.to_csv('results of models/feature importance random forest, pca, 15y train test.csv')\n",
    "        ypred.to_csv('results of models/forecast random forest, pca, 15y train test.csv')\n",
    "        ytrue.to_csv('results of models/true values random forest, pca, 15y train test.csv')\n",
    "    \n",
    "\n",
    "    # for other datasets with larger mutual information threshold, we don't apply PCA and train the model directly \n",
    "    else:\n",
    "        accuracies = {col: [] for col in Yw.columns}\n",
    "        params_by_target = {col: [] for col in Yw.columns}\n",
    "        feature_importance = {col: [] for col in Yw.columns}\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for start in tqdm(range(0, len(Xw) - window_train - window_pred + 1, window_pred)):\n",
    "            end_train = start + window_train\n",
    "            end_pred = end_train + window_pred\n",
    "\n",
    "            Xw_train = Xw.iloc[start:end_train]\n",
    "            Yw_train = Yw.iloc[start:end_train]\n",
    "            Xw_test = Xw.iloc[end_train:end_pred]\n",
    "            Yw_test = Yw.iloc[end_train:end_pred]\n",
    "\n",
    "            y_true.append(Yw_test)\n",
    "\n",
    "            grid = GridSearchCV(pipe,\n",
    "                                param_grid,\n",
    "                                cv=tscv,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "            grid.fit(Xw_train, Yw_train)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            Yw_pred = best_model.predict(Xw_test)\n",
    "            y_pred.append(Yw_pred)\n",
    "\n",
    "            for i, col in enumerate(Yw_train.columns):\n",
    "                acc = accuracy_score(Yw_test[col], Yw_pred[:, i])\n",
    "                accuracies[col].append(acc)\n",
    "            \n",
    "\n",
    "                # feature importance pour la i-ème sortie\n",
    "                est = best_model.named_steps['rf'].estimators_[i]  \n",
    "                importance = est.feature_importances_\n",
    "                feature_importance[col].append(importance)\n",
    "\n",
    "                # hyperparamètres de l'estimateur i\n",
    "                params = est.get_params()\n",
    "                params_by_target[col].append(params)\n",
    "\n",
    "\n",
    "        \n",
    "        acc = pd.DataFrame(accuracies,columns = Yw.columns)\n",
    "        params = pd.DataFrame(params_by_target, columns = Yw.columns) \n",
    "        feature_imp = pd.DataFrame(feature_importance, columns = Yw.columns)\n",
    "        Y_true_flat = np.array(y_true).reshape(-1, np.array(y_true).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ytrue = pd.DataFrame(Y_true_flat, index = Yw.iloc[780:Yw.shape[0]-1].index,columns=Yw.columns)\n",
    "        Y_pred_flat = np.array(y_pred).reshape(-1, np.array(y_pred).shape[2])  # aplati les 2 premières dimensions en une seule\n",
    "        ypred = pd.DataFrame(Y_pred_flat,index = Yw.iloc[780:Yw.shape[0]-1].index, columns=Yw.columns)\n",
    "\n",
    "        print(f'For each yield, the average out-of-sample accuracy over the 79 rolling windows on the dataset with mutual information > {threshold} is:')\n",
    "        print(acc.mean())\n",
    "        acc.to_csv(f'results of models/accuracies random forest, MI {threshold}, 15y train test.csv')\n",
    "        params.to_csv(f'results of models/params random forest, MI {threshold}, 15y train test.csv')\n",
    "        feature_imp.to_csv(f'results of models/feature importance random forest, MI {threshold}, 15y train test.csv')\n",
    "        ypred.to_csv(f'results of models/forecast random forest, MI {threshold}, 15y train test.csv')\n",
    "        ytrue.to_csv(f'results of models/true values random forest, MI {threshold}, 15y train test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64796f97",
   "metadata": {},
   "source": [
    "# IV - Models analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421e5a3",
   "metadata": {},
   "source": [
    "Now, we can observe and analyze the average out-of-sample accuracy of all the models that were trained over the 79 fifteen years rolling windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_lr = {\n",
    "            \"lr 0.03\":pd.read_csv(\"results of models/accuracies logistic regression, 15y train test.csv\",index_col=0).mean(),\n",
    "             \"lr 0.03 pca\":pd.read_csv(\"results of models/accuracies logistic regression pca, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"lr 0.035\":pd.read_csv(\"results of models/accuracies logistic regression, MI 0.035, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"lr 0.04\":pd.read_csv(\"results of models/accuracies logistic regression, MI 0.04, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"lr 0.05\":pd.read_csv(\"results of models/accuracies logistic regression, MI 0.05, 15y train test.csv\", index_col=0).mean()\n",
    "             }\n",
    "accuracies_lr = pd.DataFrame(accuracies_lr)\n",
    "\n",
    "accuracies_rf = {\n",
    "            \"rf 0.03\":pd.read_csv(\"results of models/accuracies random forest, no pca, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"rf 0.03 pca\":pd.read_csv(\"results of models/accuracies random forest pca, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"rf 0.035\":pd.read_csv(\"results of models/accuracies random forest, MI 0.035, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"rf 0.04\":pd.read_csv(\"results of models/accuracies random forest, MI 0.04, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"rf 0.05\":pd.read_csv(\"results of models/accuracies random forest, MI 0.05, 15y train test.csv\", index_col=0).mean()\n",
    "             }\n",
    "accuracies_rf = pd.DataFrame(accuracies_rf)\n",
    "\n",
    "accuracies_xg = {\n",
    "    ################ ENLEVER LES # QUAND AUDRIC AURA PUSH LES TRUCS ###########\n",
    "           # \"xg 0.03\":pd.read_csv(\"results of models/accuracies xgboost, no pca, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"xg 0.03 pca\":pd.read_csv(\"results of models/accuracies xgboost, pca, 15y train test.csv\", index_col=0).mean(),\n",
    "           #  \"xg 0.035\":pd.read_csv(\"results of models/accuracies xgboost, MI 0.035, 15y train test.csv\").mean(),\n",
    "             \"xg 0.04\":pd.read_csv(\"results of models/accuracies xgboost, MI 0.04, 15y train test.csv\", index_col=0).mean(),\n",
    "             \"xg 0.05\":pd.read_csv(\"results of models/accuracies xgboost, MI 0.05, 15y train test.csv\", index_col=0).mean()\n",
    "             }\n",
    "accuracies_xg = pd.DataFrame(accuracies_xg)\n",
    "\n",
    "\n",
    "# AUDRIC PEUX TU STP FAIRE LA MEME AVEC TES DATASETS LSTM \n",
    "#accuracies_lstm ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfa0a6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr 0.03</th>\n",
       "      <th>lr 0.03 pca</th>\n",
       "      <th>lr 0.035</th>\n",
       "      <th>lr 0.04</th>\n",
       "      <th>lr 0.05</th>\n",
       "      <th>rf 0.03</th>\n",
       "      <th>rf 0.03 pca</th>\n",
       "      <th>rf 0.035</th>\n",
       "      <th>rf 0.04</th>\n",
       "      <th>rf 0.05</th>\n",
       "      <th>xg 0.03 pca</th>\n",
       "      <th>xg 0.04</th>\n",
       "      <th>xg 0.05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1m yield</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3m yield</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6m yield</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1y yield</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2y yield</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3y yield</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5y yield</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7y yield</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10y yield</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20y yield</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30y yield</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lr 0.03  lr 0.03 pca  lr 0.035  lr 0.04  lr 0.05  rf 0.03  \\\n",
       "1m yield      0.58         0.61      0.62     0.64     0.63     0.64   \n",
       "3m yield      0.68         0.67      0.71     0.71     0.65     0.70   \n",
       "6m yield      0.64         0.63      0.68     0.65     0.64     0.67   \n",
       "1y yield      0.57         0.57      0.62     0.62     0.59     0.59   \n",
       "2y yield      0.56         0.59      0.58     0.54     0.55     0.56   \n",
       "3y yield      0.55         0.55      0.53     0.54     0.53     0.53   \n",
       "5y yield      0.50         0.55      0.55     0.54     0.50     0.54   \n",
       "7y yield      0.55         0.53      0.53     0.53     0.51     0.54   \n",
       "10y yield     0.54         0.52      0.48     0.48     0.48     0.51   \n",
       "20y yield     0.50         0.51      0.51     0.49     0.43     0.49   \n",
       "30y yield     0.53         0.51      0.48     0.49     0.46     0.48   \n",
       "\n",
       "           rf 0.03 pca  rf 0.035  rf 0.04  rf 0.05  xg 0.03 pca  xg 0.04  \\\n",
       "1m yield          0.61      0.64     0.64     0.65         0.59     0.61   \n",
       "3m yield          0.62      0.70     0.70     0.69         0.55     0.69   \n",
       "6m yield          0.57      0.67     0.67     0.67         0.50     0.67   \n",
       "1y yield          0.54      0.59     0.59     0.60         0.56     0.52   \n",
       "2y yield          0.54      0.56     0.53     0.55         0.53     0.52   \n",
       "3y yield          0.53      0.53     0.55     0.56         0.51     0.49   \n",
       "5y yield          0.56      0.51     0.52     0.52         0.51     0.50   \n",
       "7y yield          0.54      0.51     0.52     0.55         0.50     0.50   \n",
       "10y yield         0.52      0.49     0.50     0.53         0.51     0.47   \n",
       "20y yield         0.53      0.47     0.49     0.53         0.52     0.50   \n",
       "30y yield         0.53      0.47     0.49     0.49         0.51     0.50   \n",
       "\n",
       "           xg 0.05  \n",
       "1m yield      0.62  \n",
       "3m yield      0.68  \n",
       "6m yield      0.66  \n",
       "1y yield      0.56  \n",
       "2y yield      0.53  \n",
       "3y yield      0.50  \n",
       "5y yield      0.49  \n",
       "7y yield      0.51  \n",
       "10y yield     0.50  \n",
       "20y yield     0.51  \n",
       "30y yield     0.50  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = pd.concat([accuracies_lr,accuracies_rf,accuracies_xg],axis = 1).round(2)\n",
    "accuracies.index = [\"1m yield\",\"3m yield\",\"6m yield\",\"1y yield\",\"2y yield\",\"3y yield\",\"5y yield\",\"7y yield\",\"10y yield\",\"20y yield\",\"30y yield\"]\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257f82e",
   "metadata": {},
   "source": [
    "For each maturity in the yield curve, we will identify the model providing the best average out-of-sample accuracy over the 79 15 years rolling training periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6db92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best model</th>\n",
       "      <th>Best average out-of-sample accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1m yield</th>\n",
       "      <td>rf 0.05</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3m yield</th>\n",
       "      <td>lr 0.035</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6m yield</th>\n",
       "      <td>lr 0.035</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1y yield</th>\n",
       "      <td>lr 0.035</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2y yield</th>\n",
       "      <td>lr 0.03 pca</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3y yield</th>\n",
       "      <td>rf 0.05</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5y yield</th>\n",
       "      <td>rf 0.03 pca</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7y yield</th>\n",
       "      <td>lr 0.03</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10y yield</th>\n",
       "      <td>lr 0.03</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20y yield</th>\n",
       "      <td>rf 0.03 pca</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30y yield</th>\n",
       "      <td>lr 0.03</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Best model  Best average out-of-sample accuracy\n",
       "1m yield       rf 0.05                                 0.65\n",
       "3m yield      lr 0.035                                 0.71\n",
       "6m yield      lr 0.035                                 0.68\n",
       "1y yield      lr 0.035                                 0.62\n",
       "2y yield   lr 0.03 pca                                 0.59\n",
       "3y yield       rf 0.05                                 0.56\n",
       "5y yield   rf 0.03 pca                                 0.56\n",
       "7y yield       lr 0.03                                 0.55\n",
       "10y yield      lr 0.03                                 0.54\n",
       "20y yield  rf 0.03 pca                                 0.53\n",
       "30y yield      lr 0.03                                 0.53"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = pd.DataFrame({\"Best model\":accuracies.idxmax(axis=1),\n",
    "                            \"Best average out-of-sample accuracy\": accuracies.max(axis=1)\n",
    "                            })\n",
    "best_models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a220f",
   "metadata": {},
   "source": [
    "##### Some important observations on the models, the features used and the overall accuracy:\n",
    "\n",
    "- we can see that in most cases, logistic regression is the model performing the highest average out-of-sample accuracy. 7 out of 11 yield maturities are better forecast with logistic regression.\n",
    "\n",
    "- Concerning the 4 other yield maturities that are not better forecast with logistic regression, the model with the highest average out-of-sample accuracy is the random forest.\n",
    "\n",
    "- Overall, our models manage to get a satisfying out-of-sample accuracy on the short-end of the yield curve, i.e. on yields of bonds with maturity in less than 2 years: for maturities of one year or less, we manage to get average out-of-sample accuracies above 60% and even up to 71%. We are rather satisfied with these figures and these models could be useful signals to implement trading strategies. However we could not implement a strategy as we could not access data on US bond prices.\n",
    "\n",
    "- It is more difficult to forecast the medium-end and long-end of the yield curve (yields of bonds with maturity above 1y): for these yields, we get an average out-of-sample accuracy between 0.53 and 0.59. This is less satisfying and our models provide a weaker signal on those yields. The lower predictive power of our models on the long-end of the yield curve is not surprising, because long-term yields are not as much driven by macroeconomic, financial and monetary variables as short-term yields. Long-term yields mostly depend on expectations of future short rates over many years, which are latent and difficult to model and observe.\n",
    "\n",
    "- Interestingly, the random forest sometimes performs better than logistic regression to forecast the long-end of the yield curve. This could suggest that there is some non-linear relationship between some features and the long-term yields that is not captured very well by a linear model like logistic regression.\n",
    "\n",
    "- Even if in some cases, random forests provide a higher out-of-sample accuracy than linear regression, this improvement is only marginal: for instance, to forecast the 5y yield, logistic regression has a 55% out-of-sample accuracy and random forest 56%. This improvement may be marginal compared to the increase in complexity and the loss of interpretability of the model.\n",
    "\n",
    "- For most yields, the datasets containing features with mutual information above 0.03 or 0.035 are the datasets providing the best trained models. These are the datasets containing a set of macroeconomic and financial variables, as well as functions of the past yields (lags, rolling means, quantiles, ...).\n",
    "\n",
    "- However, when it comes to the shortest maturity yield, ie the 1 month yield, we see that the dataset providing the trained model with the best out-of-sample accuracy is the dataset containing features with mutual information above 0.05. This dataset contains only functions of the past yield, so it is interesting to see that we can obtain a 65% out-of-sample accuracy to predict 1 month yields only with previous values of the yields. This could suggest that the trend-following and mean-reverting behavior of the 1 month yield is stronger than for other yields, and that it could be suited for trading strategies exploiting this behavior.\n",
    "\n",
    "\n",
    "We'll now perform a Diebold Mariano test to ensure that our forecast performance is statistically significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59fc628d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m forecast_lr \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr 0.03\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast logistic regression, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m      3\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr 0.03 pca\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast logistic regression pca, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr 0.05\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast logistic regression, MI 0.05, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m              }\n\u001b[1;32m----> 8\u001b[0m forecast_lr \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m forecast_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf 0.03\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast random forest, no pca, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     12\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf 0.03 pca\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast random forest pca, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf 0.05\u001b[39m\u001b[38;5;124m\"\u001b[39m:pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults of models/forecast random forest, MI 0.05, 15y train test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m              }\n\u001b[0;32m     17\u001b[0m forecast_rf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(forecast_rf)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "forecast_lr = {\n",
    "            \"lr 0.03\":pd.read_csv(\"results of models/forecast logistic regression, 15y train test.csv\",index_col=0),\n",
    "             \"lr 0.03 pca\":pd.read_csv(\"results of models/forecast logistic regression pca, 15y train test.csv\", index_col=0),\n",
    "             \"lr 0.035\":pd.read_csv(\"results of models/forecast logistic regression, MI 0.035, 15y train test.csv\", index_col=0),\n",
    "             \"lr 0.04\":pd.read_csv(\"results of models/forecast logistic regression, MI 0.04, 15y train test.csv\", index_col=0),\n",
    "             \"lr 0.05\":pd.read_csv(\"results of models/forecast logistic regression, MI 0.05, 15y train test.csv\", index_col=0)\n",
    "             }\n",
    "forecast_lr = pd.DataFrame(forecast_lr)\n",
    "\n",
    "forecast_rf = {\n",
    "            \"rf 0.03\":pd.read_csv(\"results of models/forecast random forest, no pca, 15y train test.csv\", index_col=0),\n",
    "             \"rf 0.03 pca\":pd.read_csv(\"results of models/forecast random forest pca, 15y train test.csv\", index_col=0),\n",
    "             \"rf 0.035\":pd.read_csv(\"results of models/forecast random forest, MI 0.035, 15y train test.csv\", index_col=0),\n",
    "             \"rf 0.04\":pd.read_csv(\"results of models/forecast random forest, MI 0.04, 15y train test.csv\", index_col=0),\n",
    "             \"rf 0.05\":pd.read_csv(\"results of models/forecast random forest, MI 0.05, 15y train test.csv\", index_col=0)\n",
    "             }\n",
    "forecast_rf = pd.DataFrame(forecast_rf)\n",
    "\n",
    "forecast_xg = {\n",
    "    ################ ENLEVER LES # QUAND AUDRIC AURA PUSH LES TRUCS ###########\n",
    "           # \"xg 0.03\":pd.read_csv(\"results of models/forecast xgboost, no pca, 15y train test.csv\", index_col=0),\n",
    "             \"xg 0.03 pca\":pd.read_csv(\"results of models/forecast xgboost, pca, 15y train test.csv\", index_col=0),\n",
    "           #  \"xg 0.035\":pd.read_csv(\"results of models/forecast xgboost, MI 0.035, 15y train test.csv\"),\n",
    "             \"xg 0.04\":pd.read_csv(\"results of models/forecast xgboost, MI 0.04, 15y train test.csv\", index_col=0),\n",
    "             \"xg 0.05\":pd.read_csv(\"results of models/forecast xgboost, MI 0.05, 15y train test.csv\", index_col=0)\n",
    "             }\n",
    "forecast_xg = pd.DataFrame(forecast_xg)\n",
    "\n",
    "\n",
    "# AUDRIC PEUX TU STP FAIRE LA MEME AVEC TES DATASETS LSTM \n",
    "#forecast_lstm ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c1391c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr 0.03':             Y_DGS1MO  Y_DGS3MO  Y_DGS6MO  Y_DGS1  Y_DGS2  Y_DGS3  Y_DGS5  \\\n",
       " 2018-12-07       1.0       1.0       1.0     1.0     0.0     1.0     0.0   \n",
       " 2018-12-14       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-21       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-28       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2019-01-04       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " ...              ...       ...       ...     ...     ...     ...     ...   \n",
       " 2024-11-22       0.0       0.0       0.0     0.0     0.0     1.0     1.0   \n",
       " 2024-11-29       0.0       0.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2024-12-06       1.0       0.0       0.0     0.0     1.0     1.0     0.0   \n",
       " 2024-12-13       1.0       0.0       0.0     0.0     0.0     0.0     0.0   \n",
       " 2024-12-20       1.0       0.0       0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "             Y_DGS7  Y_DGS10  Y_DGS20  Y_DGS30  \n",
       " 2018-12-07     0.0      0.0      1.0      1.0  \n",
       " 2018-12-14     1.0      1.0      1.0      1.0  \n",
       " 2018-12-21     1.0      1.0      1.0      1.0  \n",
       " 2018-12-28     1.0      1.0      1.0      1.0  \n",
       " 2019-01-04     0.0      0.0      0.0      0.0  \n",
       " ...            ...      ...      ...      ...  \n",
       " 2024-11-22     1.0      1.0      0.0      1.0  \n",
       " 2024-11-29     0.0      1.0      1.0      1.0  \n",
       " 2024-12-06     0.0      0.0      0.0      0.0  \n",
       " 2024-12-13     0.0      0.0      0.0      0.0  \n",
       " 2024-12-20     0.0      0.0      0.0      0.0  \n",
       " \n",
       " [316 rows x 11 columns],\n",
       " 'lr 0.03 pca':             Y_DGS1MO  Y_DGS3MO  Y_DGS6MO  Y_DGS1  Y_DGS2  Y_DGS3  Y_DGS5  \\\n",
       " 2018-12-07       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2018-12-14       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-21       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2018-12-28       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2019-01-04       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " ...              ...       ...       ...     ...     ...     ...     ...   \n",
       " 2024-11-22       0.0       0.0       0.0     0.0     0.0     0.0     0.0   \n",
       " 2024-11-29       0.0       0.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2024-12-06       1.0       1.0       0.0     0.0     1.0     1.0     0.0   \n",
       " 2024-12-13       1.0       0.0       0.0     0.0     0.0     0.0     0.0   \n",
       " 2024-12-20       0.0       0.0       0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "             Y_DGS7  Y_DGS10  Y_DGS20  Y_DGS30  \n",
       " 2018-12-07     0.0      0.0      0.0      1.0  \n",
       " 2018-12-14     1.0      1.0      0.0      1.0  \n",
       " 2018-12-21     1.0      1.0      1.0      1.0  \n",
       " 2018-12-28     1.0      1.0      1.0      1.0  \n",
       " 2019-01-04     0.0      0.0      0.0      0.0  \n",
       " ...            ...      ...      ...      ...  \n",
       " 2024-11-22     0.0      0.0      0.0      1.0  \n",
       " 2024-11-29     0.0      0.0      0.0      1.0  \n",
       " 2024-12-06     0.0      0.0      0.0      0.0  \n",
       " 2024-12-13     0.0      0.0      0.0      0.0  \n",
       " 2024-12-20     0.0      0.0      0.0      0.0  \n",
       " \n",
       " [316 rows x 11 columns],\n",
       " 'lr 0.035':             Y_DGS1MO  Y_DGS3MO  Y_DGS6MO  Y_DGS1  Y_DGS2  Y_DGS3  Y_DGS5  \\\n",
       " 2018-12-07       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2018-12-14       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-21       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-28       0.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2019-01-04       0.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " ...              ...       ...       ...     ...     ...     ...     ...   \n",
       " 2024-11-22       0.0       0.0       0.0     1.0     1.0     0.0     0.0   \n",
       " 2024-11-29       0.0       0.0       1.0     1.0     1.0     0.0     0.0   \n",
       " 2024-12-06       0.0       0.0       0.0     1.0     1.0     0.0     0.0   \n",
       " 2024-12-13       1.0       0.0       0.0     1.0     1.0     0.0     0.0   \n",
       " 2024-12-20       1.0       0.0       0.0     1.0     0.0     0.0     0.0   \n",
       " \n",
       "             Y_DGS7  Y_DGS10  Y_DGS20  Y_DGS30  \n",
       " 2018-12-07     0.0      0.0      0.0      0.0  \n",
       " 2018-12-14     1.0      1.0      1.0      1.0  \n",
       " 2018-12-21     1.0      1.0      1.0      1.0  \n",
       " 2018-12-28     1.0      1.0      1.0      1.0  \n",
       " 2019-01-04     0.0      0.0      0.0      0.0  \n",
       " ...            ...      ...      ...      ...  \n",
       " 2024-11-22     0.0      1.0      1.0      1.0  \n",
       " 2024-11-29     0.0      1.0      0.0      1.0  \n",
       " 2024-12-06     0.0      1.0      0.0      0.0  \n",
       " 2024-12-13     0.0      1.0      0.0      0.0  \n",
       " 2024-12-20     0.0      1.0      0.0      0.0  \n",
       " \n",
       " [316 rows x 11 columns],\n",
       " 'lr 0.04':             Y_DGS1MO  Y_DGS3MO  Y_DGS6MO  Y_DGS1  Y_DGS2  Y_DGS3  Y_DGS5  \\\n",
       " 2018-12-07       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2018-12-14       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-21       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-28       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2019-01-04       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " ...              ...       ...       ...     ...     ...     ...     ...   \n",
       " 2024-11-22       0.0       0.0       1.0     0.0     1.0     1.0     1.0   \n",
       " 2024-11-29       0.0       0.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2024-12-06       0.0       1.0       0.0     0.0     0.0     0.0     0.0   \n",
       " 2024-12-13       1.0       0.0       0.0     1.0     1.0     1.0     0.0   \n",
       " 2024-12-20       1.0       0.0       0.0     0.0     1.0     1.0     0.0   \n",
       " \n",
       "             Y_DGS7  Y_DGS10  Y_DGS20  Y_DGS30  \n",
       " 2018-12-07     0.0      0.0      0.0      0.0  \n",
       " 2018-12-14     1.0      0.0      1.0      1.0  \n",
       " 2018-12-21     1.0      1.0      1.0      1.0  \n",
       " 2018-12-28     1.0      1.0      1.0      1.0  \n",
       " 2019-01-04     1.0      0.0      0.0      0.0  \n",
       " ...            ...      ...      ...      ...  \n",
       " 2024-11-22     0.0      1.0      1.0      1.0  \n",
       " 2024-11-29     0.0      0.0      0.0      1.0  \n",
       " 2024-12-06     0.0      0.0      0.0      1.0  \n",
       " 2024-12-13     0.0      0.0      0.0      1.0  \n",
       " 2024-12-20     0.0      0.0      0.0      1.0  \n",
       " \n",
       " [316 rows x 11 columns],\n",
       " 'lr 0.05':             Y_DGS1MO  Y_DGS3MO  Y_DGS6MO  Y_DGS1  Y_DGS2  Y_DGS3  Y_DGS5  \\\n",
       " 2018-12-07       1.0       1.0       1.0     1.0     1.0     1.0     0.0   \n",
       " 2018-12-14       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-21       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2018-12-28       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " 2019-01-04       1.0       1.0       1.0     1.0     1.0     1.0     1.0   \n",
       " ...              ...       ...       ...     ...     ...     ...     ...   \n",
       " 2024-11-22       0.0       1.0       1.0     1.0     0.0     0.0     0.0   \n",
       " 2024-11-29       1.0       1.0       1.0     1.0     0.0     0.0     0.0   \n",
       " 2024-12-06       1.0       1.0       1.0     1.0     1.0     0.0     0.0   \n",
       " 2024-12-13       1.0       1.0       1.0     1.0     0.0     0.0     0.0   \n",
       " 2024-12-20       1.0       1.0       1.0     1.0     0.0     0.0     0.0   \n",
       " \n",
       "             Y_DGS7  Y_DGS10  Y_DGS20  Y_DGS30  \n",
       " 2018-12-07     0.0      0.0      0.0      0.0  \n",
       " 2018-12-14     1.0      1.0      1.0      0.0  \n",
       " 2018-12-21     1.0      1.0      1.0      1.0  \n",
       " 2018-12-28     1.0      1.0      1.0      1.0  \n",
       " 2019-01-04     1.0      1.0      1.0      1.0  \n",
       " ...            ...      ...      ...      ...  \n",
       " 2024-11-22     0.0      1.0      1.0      1.0  \n",
       " 2024-11-29     1.0      1.0      1.0      1.0  \n",
       " 2024-12-06     1.0      1.0      1.0      1.0  \n",
       " 2024-12-13     0.0      1.0      1.0      1.0  \n",
       " 2024-12-20     0.0      0.0      0.0      0.0  \n",
       " \n",
       " [316 rows x 11 columns]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a993d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
