{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f63e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d85d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'US', 'us_data_weekly.csv')\n",
    "dus = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fda963",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1,2,5,10,15,20,25,30,40,50]:\n",
    "    dus[f'DGS1MO_t-{lag}'] = dus['DGS1MO'].shift(lag-1)\n",
    "    dus[f'DGS3MO_t-{lag}'] = dus['DGS3MO'].shift(lag-1)\n",
    "    dus[f'DGS6MO_t-{lag}'] = dus['DGS6MO'].shift(lag-1)\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "  \n",
    "\n",
    "for lag in [1,2,10]:\n",
    "    dus[f'DGS1_t-{lag}'] = dus['DGS1'].shift(lag-1)\n",
    "    dus[f'DGS2_t-{lag}'] = dus['DGS2'].shift(lag-1)\n",
    "    dus[f'DGS3_t-{lag}'] = dus['DGS3'].shift(lag-1)\n",
    "    dus[f'DGS5_t-{lag}'] = dus['DGS5'].shift(lag-1)\n",
    "    dus[f'DGS7_t-{lag}'] = dus['DGS7'].shift(lag-1)\n",
    "    dus[f'DGS10_t-{lag}'] = dus['DGS10'].shift(lag-1)\n",
    "    dus[f'DGS20_t-{lag}'] = dus['DGS20'].shift(lag-1)\n",
    "    dus[f'DGS30_t-{lag}'] = dus['DGS30'].shift(lag-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c91b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus['Y_1MO'] = dus['DGS1MO'].shift(-1)\n",
    "dus['Y_3MO'] = dus['DGS3MO'].shift(-1)\n",
    "dus['Y_6MO'] = dus['DGS6MO'].shift(-1)\n",
    "dus['Y_1year'] = dus['DGS1'].shift(-1)\n",
    "dus['Y_2year'] = dus['DGS2'].shift(-1)\n",
    "dus['Y_3year'] = dus['DGS3'].shift(-1)\n",
    "dus['Y_5year'] = dus['DGS5'].shift(-1)\n",
    "dus['Y_7year'] = dus['DGS7'].shift(-1)\n",
    "dus['Y_10year'] = dus['DGS10'].shift(-1)\n",
    "dus['Y_20year'] = dus['DGS20'].shift(-1)\n",
    "dus['Y_30year'] = dus['DGS30'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99e5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ts_features(df, cols, max_lag=30, windows=[20, 60]):\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for col in cols:\n",
    "        y = df[col]\n",
    "\n",
    "  \n",
    "        # --- Statistiques glissantes ---\n",
    "        for w in windows:\n",
    "            features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
    "            features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
    "            features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
    "            features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
    "            features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
    "            features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
    "            features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min() \n",
    "        \n",
    "       \n",
    "        # --- Autocorr√©lations locales ---\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            features[f'{col}_autocorr_{lag}'] = (\n",
    "                y.rolling(window=max(windows)).apply(lambda x: x.autocorr(lag=lag), raw=False)\n",
    "            )\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88e64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_mean_{w}'] = y.rolling(w).mean()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_std_{w}'] = y.rolling(w).std()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q25_{w}'] = y.rolling(w).quantile(0.25)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q75_{w}'] = y.rolling(w).quantile(0.75)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q05_{w}'] = y.rolling(w).quantile(0.1)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_q90_{w}'] = y.rolling(w).quantile(0.9)\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_range_{w}'] = y.rolling(w).max() - y.rolling(w).min()\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n",
      "/var/folders/l4/rqys14ws2955nprxbdsj722r0000gn/T/ipykernel_37268/2986023610.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f'{col}_autocorr_{lag}'] = (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DGS1MO_mean_20</th>\n",
       "      <th>DGS1MO_std_20</th>\n",
       "      <th>DGS1MO_q25_20</th>\n",
       "      <th>DGS1MO_q75_20</th>\n",
       "      <th>DGS1MO_q05_20</th>\n",
       "      <th>DGS1MO_q90_20</th>\n",
       "      <th>DGS1MO_range_20</th>\n",
       "      <th>DGS1MO_mean_60</th>\n",
       "      <th>DGS1MO_std_60</th>\n",
       "      <th>DGS1MO_q25_60</th>\n",
       "      <th>...</th>\n",
       "      <th>DGS30_autocorr_21</th>\n",
       "      <th>DGS30_autocorr_22</th>\n",
       "      <th>DGS30_autocorr_23</th>\n",
       "      <th>DGS30_autocorr_24</th>\n",
       "      <th>DGS30_autocorr_25</th>\n",
       "      <th>DGS30_autocorr_26</th>\n",
       "      <th>DGS30_autocorr_27</th>\n",
       "      <th>DGS30_autocorr_28</th>\n",
       "      <th>DGS30_autocorr_29</th>\n",
       "      <th>DGS30_autocorr_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-02-27</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.049001</td>\n",
       "      <td>-0.0150</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.003833</td>\n",
       "      <td>0.054309</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259523</td>\n",
       "      <td>-0.077259</td>\n",
       "      <td>0.159453</td>\n",
       "      <td>-0.306692</td>\n",
       "      <td>-0.080479</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>-0.010549</td>\n",
       "      <td>-0.041780</td>\n",
       "      <td>-0.080879</td>\n",
       "      <td>0.245651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-05</th>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.048976</td>\n",
       "      <td>-0.0150</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.003667</td>\n",
       "      <td>0.054305</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280439</td>\n",
       "      <td>-0.077388</td>\n",
       "      <td>0.157691</td>\n",
       "      <td>-0.292699</td>\n",
       "      <td>-0.092806</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>-0.032955</td>\n",
       "      <td>-0.136969</td>\n",
       "      <td>0.266099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-12</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.048839</td>\n",
       "      <td>-0.0150</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>0.054332</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350001</td>\n",
       "      <td>-0.033358</td>\n",
       "      <td>0.181366</td>\n",
       "      <td>-0.272465</td>\n",
       "      <td>-0.085109</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.049654</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>-0.149527</td>\n",
       "      <td>0.341943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-19</th>\n",
       "      <td>-0.0015</td>\n",
       "      <td>0.048480</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>0.054332</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334521</td>\n",
       "      <td>-0.013463</td>\n",
       "      <td>0.199492</td>\n",
       "      <td>-0.269040</td>\n",
       "      <td>-0.063045</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>-0.122728</td>\n",
       "      <td>0.304756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-26</th>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.048501</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.003833</td>\n",
       "      <td>0.054184</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334980</td>\n",
       "      <td>-0.012323</td>\n",
       "      <td>0.200922</td>\n",
       "      <td>-0.262623</td>\n",
       "      <td>-0.048590</td>\n",
       "      <td>0.161128</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>-0.139026</td>\n",
       "      <td>0.289036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-06</th>\n",
       "      <td>-0.0455</td>\n",
       "      <td>0.092365</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.017167</td>\n",
       "      <td>0.061261</td>\n",
       "      <td>-0.0325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139276</td>\n",
       "      <td>-0.231618</td>\n",
       "      <td>-0.123862</td>\n",
       "      <td>0.206619</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.407258</td>\n",
       "      <td>0.122133</td>\n",
       "      <td>-0.091456</td>\n",
       "      <td>0.147711</td>\n",
       "      <td>-0.167988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-13</th>\n",
       "      <td>-0.0530</td>\n",
       "      <td>0.093702</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.018833</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>-0.0325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>-0.243816</td>\n",
       "      <td>-0.109734</td>\n",
       "      <td>0.163261</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.231416</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>0.172196</td>\n",
       "      <td>-0.160077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-20</th>\n",
       "      <td>-0.0555</td>\n",
       "      <td>0.091449</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.019000</td>\n",
       "      <td>0.063157</td>\n",
       "      <td>-0.0325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084048</td>\n",
       "      <td>-0.232134</td>\n",
       "      <td>-0.108906</td>\n",
       "      <td>0.163118</td>\n",
       "      <td>0.034730</td>\n",
       "      <td>0.248380</td>\n",
       "      <td>-0.020838</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.183212</td>\n",
       "      <td>-0.152127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>-0.0550</td>\n",
       "      <td>0.091795</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.018167</td>\n",
       "      <td>0.063205</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>-0.160140</td>\n",
       "      <td>-0.070777</td>\n",
       "      <td>0.211115</td>\n",
       "      <td>0.059892</td>\n",
       "      <td>0.227114</td>\n",
       "      <td>-0.020601</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>0.176823</td>\n",
       "      <td>-0.102448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03</th>\n",
       "      <td>-0.0545</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.018167</td>\n",
       "      <td>0.063205</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013543</td>\n",
       "      <td>-0.139118</td>\n",
       "      <td>-0.065137</td>\n",
       "      <td>0.214143</td>\n",
       "      <td>0.053429</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>-0.027472</td>\n",
       "      <td>-0.071768</td>\n",
       "      <td>0.177362</td>\n",
       "      <td>-0.109288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows √ó 484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DGS1MO_mean_20  DGS1MO_std_20  DGS1MO_q25_20  DGS1MO_q75_20  \\\n",
       "2004-02-27          0.0030       0.049001        -0.0150         0.0300   \n",
       "2004-03-05          0.0025       0.048976        -0.0150         0.0300   \n",
       "2004-03-12          0.0020       0.048839        -0.0150         0.0300   \n",
       "2004-03-19         -0.0015       0.048480        -0.0300         0.0225   \n",
       "2004-03-26         -0.0005       0.048501        -0.0300         0.0225   \n",
       "...                    ...            ...            ...            ...   \n",
       "2024-12-06         -0.0455       0.092365        -0.1075         0.0125   \n",
       "2024-12-13         -0.0530       0.093702        -0.1300         0.0050   \n",
       "2024-12-20         -0.0555       0.091449        -0.1300         0.0000   \n",
       "2024-12-27         -0.0550       0.091795        -0.1300         0.0025   \n",
       "2025-01-03         -0.0545       0.092080        -0.1300         0.0025   \n",
       "\n",
       "            DGS1MO_q05_20  DGS1MO_q90_20  DGS1MO_range_20  DGS1MO_mean_60  \\\n",
       "2004-02-27         -0.052          0.042             0.23       -0.003833   \n",
       "2004-03-05         -0.052          0.042             0.23       -0.003667   \n",
       "2004-03-12         -0.052          0.042             0.23       -0.003500   \n",
       "2004-03-19         -0.052          0.042             0.23       -0.003500   \n",
       "2004-03-26         -0.052          0.042             0.23       -0.003833   \n",
       "...                   ...            ...              ...             ...   \n",
       "2024-12-06         -0.145          0.041             0.39       -0.017167   \n",
       "2024-12-13         -0.145          0.041             0.39       -0.018833   \n",
       "2024-12-20         -0.145          0.031             0.39       -0.019000   \n",
       "2024-12-27         -0.145          0.031             0.39       -0.018167   \n",
       "2025-01-03         -0.145          0.031             0.39       -0.018167   \n",
       "\n",
       "            DGS1MO_std_60  DGS1MO_q25_60  ...  DGS30_autocorr_21  \\\n",
       "2004-02-27       0.054309        -0.0300  ...          -0.259523   \n",
       "2004-03-05       0.054305        -0.0300  ...          -0.280439   \n",
       "2004-03-12       0.054332        -0.0300  ...          -0.350001   \n",
       "2004-03-19       0.054332        -0.0300  ...          -0.334521   \n",
       "2004-03-26       0.054184        -0.0300  ...          -0.334980   \n",
       "...                   ...            ...  ...                ...   \n",
       "2024-12-06       0.061261        -0.0325  ...           0.139276   \n",
       "2024-12-13       0.063221        -0.0325  ...           0.085310   \n",
       "2024-12-20       0.063157        -0.0325  ...           0.084048   \n",
       "2024-12-27       0.063205        -0.0300  ...           0.007539   \n",
       "2025-01-03       0.063205        -0.0300  ...           0.013543   \n",
       "\n",
       "            DGS30_autocorr_22  DGS30_autocorr_23  DGS30_autocorr_24  \\\n",
       "2004-02-27          -0.077259           0.159453          -0.306692   \n",
       "2004-03-05          -0.077388           0.157691          -0.292699   \n",
       "2004-03-12          -0.033358           0.181366          -0.272465   \n",
       "2004-03-19          -0.013463           0.199492          -0.269040   \n",
       "2004-03-26          -0.012323           0.200922          -0.262623   \n",
       "...                       ...                ...                ...   \n",
       "2024-12-06          -0.231618          -0.123862           0.206619   \n",
       "2024-12-13          -0.243816          -0.109734           0.163261   \n",
       "2024-12-20          -0.232134          -0.108906           0.163118   \n",
       "2024-12-27          -0.160140          -0.070777           0.211115   \n",
       "2025-01-03          -0.139118          -0.065137           0.214143   \n",
       "\n",
       "            DGS30_autocorr_25  DGS30_autocorr_26  DGS30_autocorr_27  \\\n",
       "2004-02-27          -0.080479           0.071351          -0.010549   \n",
       "2004-03-05          -0.092806           0.066148          -0.007043   \n",
       "2004-03-12          -0.085109           0.104542           0.049654   \n",
       "2004-03-19          -0.063045           0.152307           0.088528   \n",
       "2004-03-26          -0.048590           0.161128           0.072734   \n",
       "...                       ...                ...                ...   \n",
       "2024-12-06           0.008628           0.407258           0.122133   \n",
       "2024-12-13          -0.001877           0.231416           0.030619   \n",
       "2024-12-20           0.034730           0.248380          -0.020838   \n",
       "2024-12-27           0.059892           0.227114          -0.020601   \n",
       "2025-01-03           0.053429           0.225963          -0.027472   \n",
       "\n",
       "            DGS30_autocorr_28  DGS30_autocorr_29  DGS30_autocorr_30  \n",
       "2004-02-27          -0.041780          -0.080879           0.245651  \n",
       "2004-03-05          -0.032955          -0.136969           0.266099  \n",
       "2004-03-12           0.013500          -0.149527           0.341943  \n",
       "2004-03-19           0.003310          -0.122728           0.304756  \n",
       "2004-03-26           0.017370          -0.139026           0.289036  \n",
       "...                       ...                ...                ...  \n",
       "2024-12-06          -0.091456           0.147711          -0.167988  \n",
       "2024-12-13           0.030235           0.172196          -0.160077  \n",
       "2024-12-20           0.009172           0.183212          -0.152127  \n",
       "2024-12-27          -0.071118           0.176823          -0.102448  \n",
       "2025-01-03          -0.071768           0.177362          -0.109288  \n",
       "\n",
       "[1089 rows x 484 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d‚Äôusage :\n",
    "cols = ['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3',\n",
    "        'DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30']\n",
    "\n",
    "dus_features = add_ts_features(dus, cols)\n",
    "dus_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa4dea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>UEMP5TO14</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>USGOOD</th>\n",
       "      <th>USCONS</th>\n",
       "      <th>MANEMP</th>\n",
       "      <th>DMANEMP</th>\n",
       "      <th>NDMANEMP</th>\n",
       "      <th>...</th>\n",
       "      <th>DGS30_autocorr_21</th>\n",
       "      <th>DGS30_autocorr_22</th>\n",
       "      <th>DGS30_autocorr_23</th>\n",
       "      <th>DGS30_autocorr_24</th>\n",
       "      <th>DGS30_autocorr_25</th>\n",
       "      <th>DGS30_autocorr_26</th>\n",
       "      <th>DGS30_autocorr_27</th>\n",
       "      <th>DGS30_autocorr_28</th>\n",
       "      <th>DGS30_autocorr_29</th>\n",
       "      <th>DGS30_autocorr_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-02-27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259523</td>\n",
       "      <td>-0.077259</td>\n",
       "      <td>0.159453</td>\n",
       "      <td>-0.306692</td>\n",
       "      <td>-0.080479</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>-0.010549</td>\n",
       "      <td>-0.041780</td>\n",
       "      <td>-0.080879</td>\n",
       "      <td>0.245651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-05</th>\n",
       "      <td>0.003610</td>\n",
       "      <td>-0.003912</td>\n",
       "      <td>0.035091</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280439</td>\n",
       "      <td>-0.077388</td>\n",
       "      <td>0.157691</td>\n",
       "      <td>-0.292699</td>\n",
       "      <td>-0.092806</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>-0.032955</td>\n",
       "      <td>-0.136969</td>\n",
       "      <td>0.266099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350001</td>\n",
       "      <td>-0.033358</td>\n",
       "      <td>0.181366</td>\n",
       "      <td>-0.272465</td>\n",
       "      <td>-0.085109</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.049654</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>-0.149527</td>\n",
       "      <td>0.341943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334521</td>\n",
       "      <td>-0.013463</td>\n",
       "      <td>0.199492</td>\n",
       "      <td>-0.269040</td>\n",
       "      <td>-0.063045</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>-0.122728</td>\n",
       "      <td>0.304756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334980</td>\n",
       "      <td>-0.012323</td>\n",
       "      <td>0.200922</td>\n",
       "      <td>-0.262623</td>\n",
       "      <td>-0.048590</td>\n",
       "      <td>0.161128</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>-0.139026</td>\n",
       "      <td>0.289036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152791</td>\n",
       "      <td>-0.267233</td>\n",
       "      <td>-0.108337</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>-0.054965</td>\n",
       "      <td>0.373189</td>\n",
       "      <td>0.083250</td>\n",
       "      <td>-0.111179</td>\n",
       "      <td>0.176063</td>\n",
       "      <td>-0.161267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-06</th>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>-0.024098</td>\n",
       "      <td>-0.032515</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139276</td>\n",
       "      <td>-0.231618</td>\n",
       "      <td>-0.123862</td>\n",
       "      <td>0.206619</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.407258</td>\n",
       "      <td>0.122133</td>\n",
       "      <td>-0.091456</td>\n",
       "      <td>0.147711</td>\n",
       "      <td>-0.167988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>-0.243816</td>\n",
       "      <td>-0.109734</td>\n",
       "      <td>0.163261</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.231416</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>0.172196</td>\n",
       "      <td>-0.160077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084048</td>\n",
       "      <td>-0.232134</td>\n",
       "      <td>-0.108906</td>\n",
       "      <td>0.163118</td>\n",
       "      <td>0.034730</td>\n",
       "      <td>0.248380</td>\n",
       "      <td>-0.020838</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.183212</td>\n",
       "      <td>-0.152127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>-0.160140</td>\n",
       "      <td>-0.070777</td>\n",
       "      <td>0.211115</td>\n",
       "      <td>0.059892</td>\n",
       "      <td>0.227114</td>\n",
       "      <td>-0.020601</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>0.176823</td>\n",
       "      <td>-0.102448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1088 rows √ó 634 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPI    INDPRO    UNRATE  UEMP5TO14    PAYEMS    USGOOD  \\\n",
       "2004-02-27  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2004-03-05  0.003610 -0.003912  0.035091  -0.000821  0.002435  0.002992   \n",
       "2004-03-12  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2004-03-19  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2004-03-26  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "...              ...       ...       ...        ...       ...       ...   \n",
       "2024-11-29  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2024-12-06  0.001065  0.010677 -0.024098  -0.032515  0.002034  0.000185   \n",
       "2024-12-13  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2024-12-20  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2024-12-27  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "\n",
       "              USCONS    MANEMP   DMANEMP  NDMANEMP  ...  DGS30_autocorr_21  \\\n",
       "2004-02-27  0.000000  0.000000  0.000000  0.000000  ...          -0.259523   \n",
       "2004-03-05  0.007140  0.000560  0.000902  0.000000  ...          -0.280439   \n",
       "2004-03-12  0.000000  0.000000  0.000000  0.000000  ...          -0.350001   \n",
       "2004-03-19  0.000000  0.000000  0.000000  0.000000  ...          -0.334521   \n",
       "2004-03-26  0.000000  0.000000  0.000000  0.000000  ...          -0.334980   \n",
       "...              ...       ...       ...       ...  ...                ...   \n",
       "2024-11-29  0.000000  0.000000  0.000000  0.000000  ...           0.152791   \n",
       "2024-12-06  0.001811 -0.000783 -0.001895  0.001031  ...           0.139276   \n",
       "2024-12-13  0.000000  0.000000  0.000000  0.000000  ...           0.085310   \n",
       "2024-12-20  0.000000  0.000000  0.000000  0.000000  ...           0.084048   \n",
       "2024-12-27  0.000000  0.000000  0.000000  0.000000  ...           0.007539   \n",
       "\n",
       "            DGS30_autocorr_22  DGS30_autocorr_23  DGS30_autocorr_24  \\\n",
       "2004-02-27          -0.077259           0.159453          -0.306692   \n",
       "2004-03-05          -0.077388           0.157691          -0.292699   \n",
       "2004-03-12          -0.033358           0.181366          -0.272465   \n",
       "2004-03-19          -0.013463           0.199492          -0.269040   \n",
       "2004-03-26          -0.012323           0.200922          -0.262623   \n",
       "...                       ...                ...                ...   \n",
       "2024-11-29          -0.267233          -0.108337           0.218529   \n",
       "2024-12-06          -0.231618          -0.123862           0.206619   \n",
       "2024-12-13          -0.243816          -0.109734           0.163261   \n",
       "2024-12-20          -0.232134          -0.108906           0.163118   \n",
       "2024-12-27          -0.160140          -0.070777           0.211115   \n",
       "\n",
       "            DGS30_autocorr_25  DGS30_autocorr_26  DGS30_autocorr_27  \\\n",
       "2004-02-27          -0.080479           0.071351          -0.010549   \n",
       "2004-03-05          -0.092806           0.066148          -0.007043   \n",
       "2004-03-12          -0.085109           0.104542           0.049654   \n",
       "2004-03-19          -0.063045           0.152307           0.088528   \n",
       "2004-03-26          -0.048590           0.161128           0.072734   \n",
       "...                       ...                ...                ...   \n",
       "2024-11-29          -0.054965           0.373189           0.083250   \n",
       "2024-12-06           0.008628           0.407258           0.122133   \n",
       "2024-12-13          -0.001877           0.231416           0.030619   \n",
       "2024-12-20           0.034730           0.248380          -0.020838   \n",
       "2024-12-27           0.059892           0.227114          -0.020601   \n",
       "\n",
       "            DGS30_autocorr_28  DGS30_autocorr_29  DGS30_autocorr_30  \n",
       "2004-02-27          -0.041780          -0.080879           0.245651  \n",
       "2004-03-05          -0.032955          -0.136969           0.266099  \n",
       "2004-03-12           0.013500          -0.149527           0.341943  \n",
       "2004-03-19           0.003310          -0.122728           0.304756  \n",
       "2004-03-26           0.017370          -0.139026           0.289036  \n",
       "...                       ...                ...                ...  \n",
       "2024-11-29          -0.111179           0.176063          -0.161267  \n",
       "2024-12-06          -0.091456           0.147711          -0.167988  \n",
       "2024-12-13           0.030235           0.172196          -0.160077  \n",
       "2024-12-20           0.009172           0.183212          -0.152127  \n",
       "2024-12-27          -0.071118           0.176823          -0.102448  \n",
       "\n",
       "[1088 rows x 634 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dus = dus.merge(dus_features, how = 'inner', left_index=True, right_index=True)\n",
    "dus = dus.dropna()\n",
    "dus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6439e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dus = dus.drop(columns=['DGS1MO', 'DGS3MO', 'DGS6MO', 'DGS1', 'DGS2', 'DGS3','DGS5', 'DGS7', 'DGS10', 'DGS20', 'DGS30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e237cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436 features were deleted\n"
     ]
    }
   ],
   "source": [
    "Y = dus[['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']]\n",
    "\n",
    "corrs = pd.DataFrame({\n",
    "    target: dus.corrwith(Y[target]) for target in Y.columns\n",
    "}).abs()  \n",
    "\n",
    "# rep√©rer les colonnes o√π la corr√©lation absolue < 0.05 pour toutes les targets\n",
    "mask = (corrs < 0.1).all(axis=1)\n",
    "low_corr_features = corrs.index[mask]\n",
    "\n",
    "# supprimer ces colonnes\n",
    "dus_filtered = dus.drop(columns=low_corr_features)\n",
    "\n",
    "print(f\"{len(low_corr_features)} features were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f545372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1088, 187)\n"
     ]
    }
   ],
   "source": [
    "print(dus_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a47ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3404da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIG\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SEQ_LEN = 12          # input sequence length in weeks\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899d448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA (placeholder)\n",
    "target_cols = ['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']\n",
    "\n",
    "# Example: df = pd.read_csv('weekly_macro_yields.csv', parse_dates=['date'], index_col='date')\n",
    "# Ensure df sorted by date ascending\n",
    "dus_filtered = dus_filtered.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0e0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PREPROCESS: scaling and sequences\n",
    "\n",
    "df = dus_filtered\n",
    "\n",
    "# split train / val / test by time (no leakage)\n",
    "n_obs = len(df)\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "train_end = int(n_obs * train_frac)\n",
    "val_end = int(n_obs * (train_frac + val_frac))\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "# Fit scalers on training data only\n",
    "feature_cols = [c for c in df.columns if c not in target_cols]\n",
    "X_scaler = StandardScaler().fit(df_train[feature_cols])\n",
    "y_scaler = StandardScaler().fit(df_train[target_cols])  # scale targets too\n",
    "\n",
    "def create_sequences(data_df, seq_len=SEQ_LEN):\n",
    "    Xs, ys = [], []\n",
    "    data_vals = data_df[feature_cols].values\n",
    "    target_vals = data_df[target_cols].values\n",
    "    n = len(data_df)\n",
    "    for i in range(n - seq_len):\n",
    "        X_seq = data_vals[i : i + seq_len]\n",
    "        y_target = target_vals[i + seq_len]   # single-step next week\n",
    "        Xs.append(X_seq)\n",
    "        ys.append(y_target)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Scale dataframes\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_cols] = X_scaler.transform(df[feature_cols])\n",
    "df_scaled[target_cols] = y_scaler.transform(df[target_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c15642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences using scaled df and then split by indices consistently\n",
    "X_all, y_all = create_sequences(df_scaled, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84805cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine sample counts matching earlier splits\n",
    "# First index in sequences corresponds to original row index seq_len\n",
    "idx_seq_start = df_scaled.index[SEQ_LEN]\n",
    "# Compute counts for train/val/test\n",
    "n_total = len(df_scaled)\n",
    "train_count = max(0, train_end - SEQ_LEN)\n",
    "val_count = max(0, val_end - SEQ_LEN) - train_count\n",
    "test_count = len(X_all) - train_count - val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb03e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(761, 187)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54f404ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (749, 12, 176) (749, 11) (163, 12, 176) (164, 12, 176)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_all[:train_count]\n",
    "y_train = y_all[:train_count]\n",
    "X_val = X_all[train_count:train_count+val_count]\n",
    "y_val = y_all[train_count:train_count+val_count]\n",
    "X_test = X_all[train_count+val_count:]\n",
    "y_test = y_all[train_count+val_count:]\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, y_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "949ec948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span>)        ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">61,696</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">715</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m176\u001b[0m)        ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ        \u001b[38;5;34m61,696\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ         \u001b[38;5;34m4,160\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             ‚îÇ           \u001b[38;5;34m715\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,571</span> (260.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,571\u001b[0m (260.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,571</span> (260.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,571\u001b[0m (260.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. MODEL (single LSTM, multi-output)\n",
    "n_features = X_train.shape[2]\n",
    "n_targets = y_train.shape[1]\n",
    "\n",
    "inp = Input(shape=(SEQ_LEN, n_features))\n",
    "x = LSTM(LSTM_UNITS, return_sequences=False)(inp)\n",
    "x = Dropout(DROPOUT)(x)\n",
    "# optional dense hidden layer\n",
    "x = Dense(64, activation='relu')(x)\n",
    "out = Dense(n_targets, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbaf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CALLBACKS\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "chk = ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.84996, saving model to best_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 1s - 28ms/step - loss: 1.0886 - mae: 0.7185 - val_loss: 0.8500 - val_mae: 0.5885\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 2: val_loss improved from 0.84996 to 0.83105, saving model to best_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - 5ms/step - loss: 0.9870 - mae: 0.6729 - val_loss: 0.8310 - val_mae: 0.5754\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.83105\n",
      "24/24 - 0s - 4ms/step - loss: 0.9538 - mae: 0.6589 - val_loss: 0.8326 - val_mae: 0.5753\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 4: val_loss improved from 0.83105 to 0.83001, saving model to best_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - 4ms/step - loss: 0.9270 - mae: 0.6495 - val_loss: 0.8300 - val_mae: 0.5736\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.83001\n",
      "24/24 - 0s - 4ms/step - loss: 0.9011 - mae: 0.6415 - val_loss: 0.8484 - val_mae: 0.5853\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.83001\n",
      "24/24 - 0s - 4ms/step - loss: 0.8842 - mae: 0.6377 - val_loss: 0.8346 - val_mae: 0.5753\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.83001\n",
      "24/24 - 0s - 4ms/step - loss: 0.8743 - mae: 0.6308 - val_loss: 0.8746 - val_mae: 0.5935\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.83001\n",
      "24/24 - 0s - 4ms/step - loss: 0.8850 - mae: 0.6356 - val_loss: 0.8554 - val_mae: 0.5840\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.83001\n",
      "24/24 - 0s - 4ms/step - loss: 0.8994 - mae: 0.6441 - val_loss: 0.8693 - val_mae: 0.5901\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 10: val_loss improved from 0.83001 to 0.82934, saving model to best_lstm.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - 4ms/step - loss: 0.8882 - mae: 0.6401 - val_loss: 0.8293 - val_mae: 0.5713\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.8193 - mae: 0.6215 - val_loss: 0.8603 - val_mae: 0.5913\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7848 - mae: 0.6129 - val_loss: 0.8814 - val_mae: 0.5965\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7532 - mae: 0.5997 - val_loss: 0.8733 - val_mae: 0.5938\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7315 - mae: 0.5903 - val_loss: 0.9112 - val_mae: 0.6105\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7198 - mae: 0.5856 - val_loss: 0.9516 - val_mae: 0.6271\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7691 - mae: 0.5979 - val_loss: 0.8871 - val_mae: 0.6083\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.8589 - mae: 0.6295 - val_loss: 0.8859 - val_mae: 0.5940\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.8510 - mae: 0.6204 - val_loss: 0.9282 - val_mae: 0.6239\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.7466 - mae: 0.5983 - val_loss: 0.9131 - val_mae: 0.6226\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.6736 - mae: 0.5611 - val_loss: 0.9630 - val_mae: 0.6487\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.6374 - mae: 0.5496 - val_loss: 0.9409 - val_mae: 0.6300\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.6056 - mae: 0.5383 - val_loss: 1.0056 - val_mae: 0.6659\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.5950 - mae: 0.5417 - val_loss: 1.0253 - val_mae: 0.6758\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.5827 - mae: 0.5395 - val_loss: 0.9511 - val_mae: 0.6310\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.82934\n",
      "24/24 - 0s - 4ms/step - loss: 0.6147 - mae: 0.5484 - val_loss: 1.0798 - val_mae: 0.7020\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# 6. TRAIN\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es, chk],\n",
    "    verbose=2,\n",
    "    shuffle=False  # keep order; optional to set True for mini-batch randomness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a18c0002",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 7. PREDICT & INVERSE TRANSFORM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_pred_scaled = \u001b[43mmodel\u001b[49m.predict(X_test)\n\u001b[32m      3\u001b[39m y_pred = y_scaler.inverse_transform(y_pred_scaled)\n\u001b[32m      4\u001b[39m y_true = y_scaler.inverse_transform(y_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. PREDICT & INVERSE TRANSFORM\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate RMSE per maturity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i, col in enumerate(target_cols):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "    mae = np.mean(np.abs(y_true[:, i] - y_pred[:, i]))\n",
    "    print(f\"{col}: RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "# Save model if desired\n",
    "model.save('lstm_multi_yield.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0897cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_ratio(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute hit ratio (directional accuracy) for multi-output regression.\n",
    "    \n",
    "    Returns:\n",
    "        hit_rates: list of hit ratios per target (in %)\n",
    "        overall:   average hit ratio across all targets\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Compute actual and predicted changes (differences)\n",
    "    true_change = np.diff(y_true, axis=0)   # shape: (n-1, n_targets)\n",
    "    pred_change = np.diff(y_pred, axis=0)   # shape: (n-1, n_targets)\n",
    "    \n",
    "    mask = (true_change != 0) & (pred_change != 0)  # only count real moves\n",
    "    correct = (np.sign(true_change) == np.sign(pred_change)) & mask\n",
    "    hit_rates = np.mean(correct, axis=0, where=mask) * 100\n",
    "    \n",
    "    # Overall hit ratio (average across targets)\n",
    "    overall = np.mean(hit_rates)\n",
    "    \n",
    "    return hit_rates.tolist(), float(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7a0165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio per target: [48.734177215189874, 45.22292993630573, 49.693251533742334, 53.70370370370371, 53.70370370370371, 51.533742331288344, 45.06172839506173, 45.39877300613497, 46.012269938650306, 50.314465408805034, 53.70370370370371]\n",
      "Overall hit ratio: 49.37%\n"
     ]
    }
   ],
   "source": [
    "per_target, overall = hit_ratio(y_true, y_pred)\n",
    "\n",
    "print(f\"Hit ratio per target: {per_target}\")\n",
    "print(f\"Overall hit ratio: {overall:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9dfe0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute changes\n",
    "y_diff = np.diff(y_all, axis=0)                  # shape: (n_samples-1, n_targets)\n",
    "\n",
    "# 2. Convert to direction: 1 = up, 0 = down\n",
    "#    (optional: exclude flat moves with a threshold)\n",
    "THRESHOLD = 0.0  # set to 0.01 to ignore tiny moves\n",
    "y_dir = (y_diff > THRESHOLD).astype(int)\n",
    "y_train = y_dir[:train_count]\n",
    "y_val = y_dir[train_count:train_count+val_count]\n",
    "y_test = y_dir[train_count+val_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c08fd547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span>)        ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">61,696</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">363</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m176\u001b[0m)        ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ        \u001b[38;5;34m61,696\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ         \u001b[38;5;34m2,080\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             ‚îÇ           \u001b[38;5;34m363\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,139</span> (250.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,139\u001b[0m (250.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,139</span> (250.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,139\u001b[0m (250.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = X_train.shape[2]\n",
    "n_targets = y_train.shape[1]\n",
    "\n",
    "inp = Input(shape=(SEQ_LEN, n_features))\n",
    "x = LSTM(LSTM_UNITS, return_sequences=False)(inp)\n",
    "x = Dropout(DROPOUT)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "# One output per target ‚Üí probability of \"UP\"\n",
    "out = Dense(n_targets, activation='sigmoid')(x)  # [0,1] per target\n",
    "\n",
    "LSTM_cls = Model(inp, out)\n",
    "LSTM_cls.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',      # perfect for 0/1 labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "LSTM_cls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72583674",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 6. TRAIN\n",
    "history = LSTM_cls.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[es, chk],\n",
    "    shuffle=False  # keep order; optional to set True for mini-batch randomness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64159ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PREPROCESS: scaling and sequences\n",
    "\n",
    "df = dus_filtered\n",
    "\n",
    "# split train / val / test by time (no leakage)\n",
    "n_obs = len(df)\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "train_end = int(n_obs * train_frac)\n",
    "val_end = int(n_obs * (train_frac + val_frac))\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "# Fit scalers on training data only\n",
    "feature_cols = [c for c in df.columns if c not in target_cols]\n",
    "X_scaler = StandardScaler().fit(df_train[feature_cols])\n",
    "y_scaler = StandardScaler().fit(df_train[target_cols])  # scale targets too\n",
    "\n",
    "def create_sequences(data_df, seq_len=SEQ_LEN):\n",
    "    Xs, ys = [], []\n",
    "    data_vals = data_df[feature_cols].values\n",
    "    target_vals = data_df[target_cols].values\n",
    "    n = len(data_df)\n",
    "    for i in range(n - seq_len):\n",
    "        X_seq = data_vals[i : i + seq_len]\n",
    "        y_target = target_vals[i + seq_len]   # single-step next week\n",
    "        Xs.append(X_seq)\n",
    "        ys.append(y_target)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Scale dataframes\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_cols] = X_scaler.transform(df[feature_cols])\n",
    "df_scaled[target_cols] = y_scaler.transform(df[target_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c654caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA (placeholder)\n",
    "df = dus_filtered.copy()\n",
    "df = df.sort_index()\n",
    "target_cols = ['Y_1MO', 'Y_3MO', 'Y_6MO', 'Y_1year', 'Y_2year', 'Y_3year', 'Y_5year', 'Y_7year', 'Y_10year', 'Y_20year', 'Y_30year']\n",
    "\n",
    "# Ensure df sorted by date ascending\n",
    "feature_cols = df.columns.difference(target_cols)\n",
    "directions = (df[target_cols].shift(-1) > df[target_cols]).astype(int)[:-1]\n",
    "features = df[feature_cols][:-1]\n",
    "\n",
    "# split train / val / test by time (no leakage)\n",
    "n_obs = len(df)\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "train_end = int(n_obs * train_frac)\n",
    "val_end = int(n_obs * (train_frac + val_frac))\n",
    "\n",
    "features_train = features.iloc[:train_end]\n",
    "features_scaler = StandardScaler().fit(features_train)\n",
    "features_scaled = features_scaler.fit_transform(features)\n",
    "\n",
    "X = features_scaled\n",
    "y = directions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d8172e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "SEQ_LEN = 10\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ab229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create PyTorch dataset\n",
    "class YieldDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_length=SEQ_LEN):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_seq = self.X[idx:idx+self.seq_length]\n",
    "        y_seq = self.y[idx+self.seq_length]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26d5304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.615384615384615"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end/52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b46aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(X)\n",
    "train_end = int(n_obs * 0.7)\n",
    "val_end = int(n_obs * 0.85)\n",
    "\n",
    "train_dataset = YieldDataset(X[:train_end], y[:train_end], SEQ_LEN)\n",
    "val_dataset   = YieldDataset(X[train_end:val_end], y[train_end:val_end], SEQ_LEN)\n",
    "test_dataset  = YieldDataset(X[val_end:], y[val_end:], SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7f6fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]  # take last time step\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "750aa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "model = LSTMClassifier(input_dim, HIDDEN_DIM, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9199e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.6113 | Val Loss: 0.7069\n",
      "Epoch 2/200 | Train Loss: 0.6029 | Val Loss: 0.7070\n",
      "Epoch 3/200 | Train Loss: 0.5922 | Val Loss: 0.7108\n",
      "Epoch 4/200 | Train Loss: 0.5816 | Val Loss: 0.7129\n",
      "Epoch 5/200 | Train Loss: 0.5702 | Val Loss: 0.7171\n",
      "Epoch 6/200 | Train Loss: 0.5580 | Val Loss: 0.7181\n",
      "Epoch 7/200 | Train Loss: 0.5469 | Val Loss: 0.7210\n",
      "Epoch 8/200 | Train Loss: 0.5350 | Val Loss: 0.7257\n",
      "Epoch 9/200 | Train Loss: 0.5237 | Val Loss: 0.7272\n",
      "Epoch 10/200 | Train Loss: 0.5107 | Val Loss: 0.7320\n",
      "Epoch 11/200 | Train Loss: 0.4985 | Val Loss: 0.7332\n",
      "Epoch 12/200 | Train Loss: 0.4831 | Val Loss: 0.7343\n",
      "Epoch 13/200 | Train Loss: 0.4711 | Val Loss: 0.7389\n",
      "Epoch 14/200 | Train Loss: 0.4581 | Val Loss: 0.7407\n",
      "Epoch 15/200 | Train Loss: 0.4458 | Val Loss: 0.7464\n",
      "Epoch 16/200 | Train Loss: 0.4336 | Val Loss: 0.7492\n",
      "Epoch 17/200 | Train Loss: 0.4218 | Val Loss: 0.7505\n",
      "Epoch 18/200 | Train Loss: 0.4112 | Val Loss: 0.7564\n",
      "Epoch 19/200 | Train Loss: 0.3976 | Val Loss: 0.7629\n",
      "Epoch 20/200 | Train Loss: 0.3870 | Val Loss: 0.7625\n",
      "Epoch 21/200 | Train Loss: 0.3765 | Val Loss: 0.7675\n",
      "Epoch 22/200 | Train Loss: 0.3680 | Val Loss: 0.7675\n",
      "Epoch 23/200 | Train Loss: 0.3591 | Val Loss: 0.7711\n",
      "Epoch 24/200 | Train Loss: 0.3505 | Val Loss: 0.7789\n",
      "Epoch 25/200 | Train Loss: 0.3425 | Val Loss: 0.7805\n",
      "Epoch 26/200 | Train Loss: 0.3324 | Val Loss: 0.7850\n",
      "Epoch 27/200 | Train Loss: 0.3248 | Val Loss: 0.7853\n",
      "Epoch 28/200 | Train Loss: 0.3192 | Val Loss: 0.7904\n",
      "Epoch 29/200 | Train Loss: 0.3109 | Val Loss: 0.7867\n",
      "Epoch 30/200 | Train Loss: 0.3041 | Val Loss: 0.7969\n",
      "Epoch 31/200 | Train Loss: 0.2983 | Val Loss: 0.7969\n",
      "Epoch 32/200 | Train Loss: 0.2931 | Val Loss: 0.7982\n",
      "Epoch 33/200 | Train Loss: 0.2859 | Val Loss: 0.8018\n",
      "Epoch 34/200 | Train Loss: 0.2811 | Val Loss: 0.8050\n",
      "Epoch 35/200 | Train Loss: 0.2749 | Val Loss: 0.8025\n",
      "Epoch 36/200 | Train Loss: 0.2690 | Val Loss: 0.8104\n",
      "Epoch 37/200 | Train Loss: 0.2636 | Val Loss: 0.8130\n",
      "Epoch 38/200 | Train Loss: 0.2597 | Val Loss: 0.8157\n",
      "Epoch 39/200 | Train Loss: 0.2554 | Val Loss: 0.8146\n",
      "Epoch 40/200 | Train Loss: 0.2496 | Val Loss: 0.8246\n",
      "Epoch 41/200 | Train Loss: 0.2446 | Val Loss: 0.8210\n",
      "Epoch 42/200 | Train Loss: 0.2397 | Val Loss: 0.8277\n",
      "Epoch 43/200 | Train Loss: 0.2366 | Val Loss: 0.8304\n",
      "Epoch 44/200 | Train Loss: 0.2324 | Val Loss: 0.8340\n",
      "Epoch 45/200 | Train Loss: 0.2275 | Val Loss: 0.8341\n",
      "Epoch 46/200 | Train Loss: 0.2239 | Val Loss: 0.8378\n",
      "Epoch 47/200 | Train Loss: 0.2197 | Val Loss: 0.8427\n",
      "Epoch 48/200 | Train Loss: 0.2166 | Val Loss: 0.8456\n",
      "Epoch 49/200 | Train Loss: 0.2125 | Val Loss: 0.8514\n",
      "Epoch 50/200 | Train Loss: 0.2091 | Val Loss: 0.8521\n",
      "Epoch 51/200 | Train Loss: 0.2060 | Val Loss: 0.8536\n",
      "Epoch 52/200 | Train Loss: 0.2026 | Val Loss: 0.8637\n",
      "Epoch 53/200 | Train Loss: 0.1987 | Val Loss: 0.8634\n",
      "Epoch 54/200 | Train Loss: 0.1958 | Val Loss: 0.8664\n",
      "Epoch 55/200 | Train Loss: 0.1920 | Val Loss: 0.8691\n",
      "Epoch 56/200 | Train Loss: 0.1882 | Val Loss: 0.8730\n",
      "Epoch 57/200 | Train Loss: 0.1855 | Val Loss: 0.8740\n",
      "Epoch 58/200 | Train Loss: 0.1812 | Val Loss: 0.8798\n",
      "Epoch 59/200 | Train Loss: 0.1788 | Val Loss: 0.8825\n",
      "Epoch 60/200 | Train Loss: 0.1760 | Val Loss: 0.8881\n",
      "Epoch 61/200 | Train Loss: 0.1733 | Val Loss: 0.8864\n",
      "Epoch 62/200 | Train Loss: 0.1703 | Val Loss: 0.8928\n",
      "Epoch 63/200 | Train Loss: 0.1672 | Val Loss: 0.9001\n",
      "Epoch 64/200 | Train Loss: 0.1639 | Val Loss: 0.8990\n",
      "Epoch 65/200 | Train Loss: 0.1613 | Val Loss: 0.9044\n",
      "Epoch 66/200 | Train Loss: 0.1589 | Val Loss: 0.9054\n",
      "Epoch 67/200 | Train Loss: 0.1559 | Val Loss: 0.9118\n",
      "Epoch 68/200 | Train Loss: 0.1529 | Val Loss: 0.9144\n",
      "Epoch 69/200 | Train Loss: 0.1508 | Val Loss: 0.9143\n",
      "Epoch 70/200 | Train Loss: 0.1477 | Val Loss: 0.9182\n",
      "Epoch 71/200 | Train Loss: 0.1458 | Val Loss: 0.9247\n",
      "Epoch 72/200 | Train Loss: 0.1428 | Val Loss: 0.9242\n",
      "Epoch 73/200 | Train Loss: 0.1409 | Val Loss: 0.9315\n",
      "Epoch 74/200 | Train Loss: 0.1388 | Val Loss: 0.9321\n",
      "Epoch 75/200 | Train Loss: 0.1360 | Val Loss: 0.9363\n",
      "Epoch 76/200 | Train Loss: 0.1338 | Val Loss: 0.9398\n",
      "Epoch 77/200 | Train Loss: 0.1309 | Val Loss: 0.9405\n",
      "Epoch 78/200 | Train Loss: 0.1285 | Val Loss: 0.9463\n",
      "Epoch 79/200 | Train Loss: 0.1256 | Val Loss: 0.9484\n",
      "Epoch 80/200 | Train Loss: 0.1242 | Val Loss: 0.9506\n",
      "Epoch 81/200 | Train Loss: 0.1218 | Val Loss: 0.9531\n",
      "Epoch 82/200 | Train Loss: 0.1199 | Val Loss: 0.9552\n",
      "Epoch 83/200 | Train Loss: 0.1175 | Val Loss: 0.9627\n",
      "Epoch 84/200 | Train Loss: 0.1165 | Val Loss: 0.9541\n",
      "Epoch 85/200 | Train Loss: 0.1142 | Val Loss: 0.9661\n",
      "Epoch 86/200 | Train Loss: 0.1114 | Val Loss: 0.9633\n",
      "Epoch 87/200 | Train Loss: 0.1090 | Val Loss: 0.9688\n",
      "Epoch 88/200 | Train Loss: 0.1077 | Val Loss: 0.9725\n",
      "Epoch 89/200 | Train Loss: 0.1059 | Val Loss: 0.9745\n",
      "Epoch 90/200 | Train Loss: 0.1050 | Val Loss: 0.9776\n",
      "Epoch 91/200 | Train Loss: 0.1034 | Val Loss: 0.9836\n",
      "Epoch 92/200 | Train Loss: 0.1014 | Val Loss: 0.9777\n",
      "Epoch 93/200 | Train Loss: 0.0992 | Val Loss: 0.9871\n",
      "Epoch 94/200 | Train Loss: 0.0974 | Val Loss: 0.9817\n",
      "Epoch 95/200 | Train Loss: 0.0960 | Val Loss: 0.9889\n",
      "Epoch 96/200 | Train Loss: 0.0935 | Val Loss: 0.9894\n",
      "Epoch 97/200 | Train Loss: 0.0925 | Val Loss: 0.9916\n",
      "Epoch 98/200 | Train Loss: 0.0906 | Val Loss: 0.9977\n",
      "Epoch 99/200 | Train Loss: 0.0893 | Val Loss: 0.9978\n",
      "Epoch 100/200 | Train Loss: 0.0877 | Val Loss: 1.0001\n",
      "Epoch 101/200 | Train Loss: 0.0869 | Val Loss: 1.0053\n",
      "Epoch 102/200 | Train Loss: 0.0852 | Val Loss: 1.0118\n",
      "Epoch 103/200 | Train Loss: 0.0840 | Val Loss: 0.9994\n",
      "Epoch 104/200 | Train Loss: 0.0825 | Val Loss: 1.0136\n",
      "Epoch 105/200 | Train Loss: 0.0807 | Val Loss: 1.0144\n",
      "Epoch 106/200 | Train Loss: 0.0795 | Val Loss: 1.0134\n",
      "Epoch 107/200 | Train Loss: 0.0783 | Val Loss: 1.0205\n",
      "Epoch 108/200 | Train Loss: 0.0765 | Val Loss: 1.0169\n",
      "Epoch 109/200 | Train Loss: 0.0753 | Val Loss: 1.0283\n",
      "Epoch 110/200 | Train Loss: 0.0740 | Val Loss: 1.0232\n",
      "Epoch 111/200 | Train Loss: 0.0726 | Val Loss: 1.0238\n",
      "Epoch 112/200 | Train Loss: 0.0711 | Val Loss: 1.0323\n",
      "Epoch 113/200 | Train Loss: 0.0699 | Val Loss: 1.0309\n",
      "Epoch 114/200 | Train Loss: 0.0687 | Val Loss: 1.0388\n",
      "Epoch 115/200 | Train Loss: 0.0681 | Val Loss: 1.0392\n",
      "Epoch 116/200 | Train Loss: 0.0665 | Val Loss: 1.0391\n",
      "Epoch 117/200 | Train Loss: 0.0652 | Val Loss: 1.0435\n",
      "Epoch 118/200 | Train Loss: 0.0641 | Val Loss: 1.0443\n",
      "Epoch 119/200 | Train Loss: 0.0630 | Val Loss: 1.0435\n",
      "Epoch 120/200 | Train Loss: 0.0623 | Val Loss: 1.0519\n",
      "Epoch 121/200 | Train Loss: 0.0611 | Val Loss: 1.0536\n",
      "Epoch 122/200 | Train Loss: 0.0602 | Val Loss: 1.0586\n",
      "Epoch 123/200 | Train Loss: 0.0595 | Val Loss: 1.0516\n",
      "Epoch 124/200 | Train Loss: 0.0587 | Val Loss: 1.0702\n",
      "Epoch 125/200 | Train Loss: 0.0580 | Val Loss: 1.0632\n",
      "Epoch 126/200 | Train Loss: 0.0573 | Val Loss: 1.0689\n",
      "Epoch 127/200 | Train Loss: 0.0563 | Val Loss: 1.0671\n",
      "Epoch 128/200 | Train Loss: 0.0549 | Val Loss: 1.0710\n",
      "Epoch 129/200 | Train Loss: 0.0539 | Val Loss: 1.0748\n",
      "Epoch 130/200 | Train Loss: 0.0528 | Val Loss: 1.0741\n",
      "Epoch 131/200 | Train Loss: 0.0520 | Val Loss: 1.0871\n",
      "Epoch 132/200 | Train Loss: 0.0513 | Val Loss: 1.0788\n",
      "Epoch 133/200 | Train Loss: 0.0503 | Val Loss: 1.0832\n",
      "Epoch 134/200 | Train Loss: 0.0493 | Val Loss: 1.0850\n",
      "Epoch 135/200 | Train Loss: 0.0488 | Val Loss: 1.0959\n",
      "Epoch 136/200 | Train Loss: 0.0479 | Val Loss: 1.0880\n",
      "Epoch 137/200 | Train Loss: 0.0471 | Val Loss: 1.0943\n",
      "Epoch 138/200 | Train Loss: 0.0462 | Val Loss: 1.1014\n",
      "Epoch 139/200 | Train Loss: 0.0457 | Val Loss: 1.0992\n",
      "Epoch 140/200 | Train Loss: 0.0448 | Val Loss: 1.1014\n",
      "Epoch 141/200 | Train Loss: 0.0442 | Val Loss: 1.1097\n",
      "Epoch 142/200 | Train Loss: 0.0436 | Val Loss: 1.1084\n",
      "Epoch 143/200 | Train Loss: 0.0430 | Val Loss: 1.1085\n",
      "Epoch 144/200 | Train Loss: 0.0421 | Val Loss: 1.1084\n",
      "Epoch 145/200 | Train Loss: 0.0412 | Val Loss: 1.1196\n",
      "Epoch 146/200 | Train Loss: 0.0407 | Val Loss: 1.1094\n",
      "Epoch 147/200 | Train Loss: 0.0402 | Val Loss: 1.1239\n",
      "Epoch 148/200 | Train Loss: 0.0395 | Val Loss: 1.1165\n",
      "Epoch 149/200 | Train Loss: 0.0396 | Val Loss: 1.1260\n",
      "Epoch 150/200 | Train Loss: 0.0392 | Val Loss: 1.1186\n",
      "Epoch 151/200 | Train Loss: 0.0384 | Val Loss: 1.1325\n",
      "Epoch 152/200 | Train Loss: 0.0377 | Val Loss: 1.1252\n",
      "Epoch 153/200 | Train Loss: 0.0372 | Val Loss: 1.1332\n",
      "Epoch 154/200 | Train Loss: 0.0365 | Val Loss: 1.1302\n",
      "Epoch 155/200 | Train Loss: 0.0357 | Val Loss: 1.1384\n",
      "Epoch 156/200 | Train Loss: 0.0351 | Val Loss: 1.1367\n",
      "Epoch 157/200 | Train Loss: 0.0344 | Val Loss: 1.1396\n",
      "Epoch 158/200 | Train Loss: 0.0337 | Val Loss: 1.1413\n",
      "Epoch 159/200 | Train Loss: 0.0334 | Val Loss: 1.1438\n",
      "Epoch 160/200 | Train Loss: 0.0328 | Val Loss: 1.1449\n",
      "Epoch 161/200 | Train Loss: 0.0323 | Val Loss: 1.1479\n",
      "Epoch 162/200 | Train Loss: 0.0319 | Val Loss: 1.1497\n",
      "Epoch 163/200 | Train Loss: 0.0314 | Val Loss: 1.1538\n",
      "Epoch 164/200 | Train Loss: 0.0309 | Val Loss: 1.1542\n",
      "Epoch 165/200 | Train Loss: 0.0304 | Val Loss: 1.1562\n",
      "Epoch 166/200 | Train Loss: 0.0300 | Val Loss: 1.1599\n",
      "Epoch 167/200 | Train Loss: 0.0295 | Val Loss: 1.1575\n",
      "Epoch 168/200 | Train Loss: 0.0289 | Val Loss: 1.1695\n",
      "Epoch 169/200 | Train Loss: 0.0288 | Val Loss: 1.1608\n",
      "Epoch 170/200 | Train Loss: 0.0281 | Val Loss: 1.1657\n",
      "Epoch 171/200 | Train Loss: 0.0280 | Val Loss: 1.1704\n",
      "Epoch 172/200 | Train Loss: 0.0273 | Val Loss: 1.1677\n",
      "Epoch 173/200 | Train Loss: 0.0271 | Val Loss: 1.1730\n",
      "Epoch 174/200 | Train Loss: 0.0270 | Val Loss: 1.1738\n",
      "Epoch 175/200 | Train Loss: 0.0270 | Val Loss: 1.1770\n",
      "Epoch 176/200 | Train Loss: 0.0268 | Val Loss: 1.1753\n",
      "Epoch 177/200 | Train Loss: 0.0275 | Val Loss: 1.1744\n",
      "Epoch 178/200 | Train Loss: 0.0268 | Val Loss: 1.1927\n",
      "Epoch 179/200 | Train Loss: 0.0259 | Val Loss: 1.1888\n",
      "Epoch 180/200 | Train Loss: 0.0253 | Val Loss: 1.1870\n",
      "Epoch 181/200 | Train Loss: 0.0250 | Val Loss: 1.1888\n",
      "Epoch 182/200 | Train Loss: 0.0245 | Val Loss: 1.1827\n",
      "Epoch 183/200 | Train Loss: 0.0239 | Val Loss: 1.1900\n",
      "Epoch 184/200 | Train Loss: 0.0235 | Val Loss: 1.1933\n",
      "Epoch 185/200 | Train Loss: 0.0231 | Val Loss: 1.1975\n",
      "Epoch 186/200 | Train Loss: 0.0228 | Val Loss: 1.1923\n",
      "Epoch 187/200 | Train Loss: 0.0224 | Val Loss: 1.2015\n",
      "Epoch 188/200 | Train Loss: 0.0221 | Val Loss: 1.1934\n",
      "Epoch 189/200 | Train Loss: 0.0216 | Val Loss: 1.2010\n",
      "Epoch 190/200 | Train Loss: 0.0213 | Val Loss: 1.2000\n",
      "Epoch 191/200 | Train Loss: 0.0210 | Val Loss: 1.2016\n",
      "Epoch 192/200 | Train Loss: 0.0207 | Val Loss: 1.2048\n",
      "Epoch 193/200 | Train Loss: 0.0205 | Val Loss: 1.2039\n",
      "Epoch 194/200 | Train Loss: 0.0202 | Val Loss: 1.2103\n",
      "Epoch 195/200 | Train Loss: 0.0200 | Val Loss: 1.2065\n",
      "Epoch 196/200 | Train Loss: 0.0197 | Val Loss: 1.2166\n",
      "Epoch 197/200 | Train Loss: 0.0194 | Val Loss: 1.2143\n",
      "Epoch 198/200 | Train Loss: 0.0190 | Val Loss: 1.2152\n",
      "Epoch 199/200 | Train Loss: 0.0187 | Val Loss: 1.2173\n",
      "Epoch 200/200 | Train Loss: 0.0184 | Val Loss: 1.2183\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {np.mean(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c1f912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per yield:\n",
      "Y_1MO: 0.455\n",
      "Y_3MO: 0.506\n",
      "Y_6MO: 0.494\n",
      "Y_1year: 0.519\n",
      "Y_2year: 0.500\n",
      "Y_3year: 0.474\n",
      "Y_5year: 0.494\n",
      "Y_7year: 0.442\n",
      "Y_10year: 0.468\n",
      "Y_20year: 0.455\n",
      "Y_30year: 0.474\n",
      "Overall accuracy: 0.480\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "y_true_list, y_pred_list = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        y_true_list.append(y_batch.numpy())\n",
    "        y_pred_list.append(preds.numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true_list)\n",
    "y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "acc_per_yield = (y_true == y_pred).mean(axis=0)\n",
    "overall_acc = (y_true == y_pred).mean()\n",
    "\n",
    "print(\"\\nAccuracy per yield:\")\n",
    "for name, acc in zip(target_cols, acc_per_yield):\n",
    "    print(f\"{name}: {acc:.3f}\")\n",
    "print(f\"Overall accuracy: {overall_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2089d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 16   # smaller\n",
    "LR = 5e-4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f8cd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "490021e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "model = RLSTMClassifier(input_dim, HIDDEN_DIM, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d295da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train 0.6988 | Val 0.7011\n",
      "Epoch 2/50 | Train 0.6966 | Val 0.7002\n",
      "Epoch 3/50 | Train 0.6962 | Val 0.6993\n",
      "Epoch 4/50 | Train 0.6949 | Val 0.6986\n",
      "Epoch 5/50 | Train 0.6942 | Val 0.6979\n",
      "Epoch 6/50 | Train 0.6932 | Val 0.6974\n",
      "Epoch 7/50 | Train 0.6920 | Val 0.6970\n",
      "Epoch 8/50 | Train 0.6918 | Val 0.6967\n",
      "Epoch 9/50 | Train 0.6909 | Val 0.6965\n",
      "Epoch 10/50 | Train 0.6901 | Val 0.6961\n",
      "Epoch 11/50 | Train 0.6893 | Val 0.6960\n",
      "Epoch 12/50 | Train 0.6882 | Val 0.6957\n",
      "Epoch 13/50 | Train 0.6883 | Val 0.6954\n",
      "Epoch 14/50 | Train 0.6886 | Val 0.6955\n",
      "Epoch 15/50 | Train 0.6880 | Val 0.6954\n",
      "Epoch 16/50 | Train 0.6892 | Val 0.6952\n",
      "Epoch 17/50 | Train 0.6868 | Val 0.6950\n",
      "Epoch 18/50 | Train 0.6873 | Val 0.6948\n",
      "Epoch 19/50 | Train 0.6859 | Val 0.6948\n",
      "Epoch 20/50 | Train 0.6862 | Val 0.6946\n",
      "Epoch 21/50 | Train 0.6849 | Val 0.6946\n",
      "Epoch 22/50 | Train 0.6843 | Val 0.6947\n",
      "Epoch 23/50 | Train 0.6850 | Val 0.6946\n",
      "Epoch 24/50 | Train 0.6834 | Val 0.6945\n",
      "Epoch 25/50 | Train 0.6836 | Val 0.6947\n",
      "Epoch 26/50 | Train 0.6832 | Val 0.6948\n",
      "Epoch 27/50 | Train 0.6827 | Val 0.6947\n",
      "Epoch 28/50 | Train 0.6824 | Val 0.6947\n",
      "Epoch 29/50 | Train 0.6827 | Val 0.6948\n",
      "‚èπÔ∏è Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience, patience_counter = 5, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train {np.mean(train_losses):.4f} | Val {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# restore best model\n",
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20579ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per yield:\n",
      "Y_1MO: 0.519\n",
      "Y_3MO: 0.552\n",
      "Y_6MO: 0.519\n",
      "Y_1year: 0.455\n",
      "Y_2year: 0.461\n",
      "Y_3year: 0.532\n",
      "Y_5year: 0.474\n",
      "Y_7year: 0.506\n",
      "Y_10year: 0.481\n",
      "Y_20year: 0.513\n",
      "Y_30year: 0.519\n",
      "Overall accuracy: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "y_true_list, y_pred_list = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        y_true_list.append(y_batch.numpy())\n",
    "        y_pred_list.append(preds.numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true_list)\n",
    "y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "acc_per_yield = (y_true == y_pred).mean(axis=0)\n",
    "overall_acc = (y_true == y_pred).mean()\n",
    "\n",
    "print(\"\\nAccuracy per yield:\")\n",
    "for name, acc in zip(target_cols, acc_per_yield):\n",
    "    print(f\"{name}: {acc:.3f}\")\n",
    "print(f\"Overall accuracy: {overall_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d060440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
